{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YTaQq9dCh0E"
   },
   "source": [
    "# Clasificación de palabras (por género de nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sumit\\\\OneDrive\\\\Escritorio\\\\Universidad\\\\P_Curriculares\\\\Clasificación de texto (NLP)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/sumit/OneDrive/Escritorio/Universidad/P_Curriculares/Clasificación de texto (NLP)/venv/Lib/site-packages\")\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1594920888413,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "5co_TuOhC4ze",
    "outputId": "6ed198ee-9cb9-48b9-ab74-db9655585c0e"
   },
   "outputs": [],
   "source": [
    "import nltk, random\n",
    "#nltk.download('names')\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijhsE2PBFYxm"
   },
   "source": [
    "**Función básica de extracción de atributos**\n",
    "\n",
    "Vamos a hacer que al principio esta función devuelva solamente como atributo la última letra del nombre de la persona ya que los humanos somos capaces de clasificar un nombre como masculino o femenino teniendo en cuenta solo este atributo.\n",
    "\n",
    "PD: el corpus es un conjunto de nombre en **inglés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "b0kKV62lCZ55"
   },
   "outputs": [],
   "source": [
    "# definición de atributos relevantes\n",
    "def atributos(palabra):\n",
    "    return {'ultima_letra': palabra[-1]}\n",
    "\n",
    "tagset = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(APRENDIZAJE SUPERVISADO)\n",
    "\n",
    "Pero esta lista tiene todos los nombres masculinos primero y luego los femeninos, lo cuál puede ser un problema al momento de entrenar el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1594920888415,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "IjfK5ZKwDL__",
    "outputId": "fef1000d-e474-43f0-986d-10b721946e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aamir', 'male'),\n",
       " ('Aaron', 'male'),\n",
       " ('Abbey', 'male'),\n",
       " ('Abbie', 'male'),\n",
       " ('Abbot', 'male'),\n",
       " ('Abbott', 'male'),\n",
       " ('Abby', 'male'),\n",
       " ('Abdel', 'male'),\n",
       " ('Abdul', 'male'),\n",
       " ('Abdulkarim', 'male')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1594920888416,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "jZcAN-dmCrok",
    "outputId": "b114852f-8e71-4121-fda3-21bb731353b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Agneta', 'female'),\n",
       " ('Dugan', 'male'),\n",
       " ('Tobi', 'female'),\n",
       " ('Axel', 'male'),\n",
       " ('Patrice', 'female'),\n",
       " ('Renault', 'male'),\n",
       " ('Rudd', 'male'),\n",
       " ('Chelsy', 'female'),\n",
       " ('Petra', 'female'),\n",
       " ('Wojciech', 'male')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(tagset)\n",
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos una lista con pares: (atributos,genero) ya que el modelo que estamos creando no lee los nombres sino los atributos de los nombres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QzK97C8BDmHR"
   },
   "outputs": [],
   "source": [
    "fset = [(atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQPv0tW4Fd2G"
   },
   "source": [
    "**Modelo de clasificación Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "37jueg4nDQFs"
   },
   "outputs": [],
   "source": [
    "# entrenamiento del modelo NaiveBayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAWfUSHrEj3q"
   },
   "source": [
    " **Verificación de algunas predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1594920888418,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "Mr8ytm8SEEZk",
    "outputId": "cf62ff8a-2722-4331-bf70-66aafff1d9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('amanda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1594920888418,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "c0GG1Y1_EPaO",
    "outputId": "6f286792-7845-44f8-b22b-1cf6815036a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('peter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea no tiene porque demostrar que esta clasificando bien, hay que mirar el performance del modelo que hemos entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('sumit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSUK14XhEqLL"
   },
   "source": [
    "**Performance del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1594920888419,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "lenwC5agEdvT",
    "outputId": "1ceb5e52-4db6-4c5f-c714-dbf72f90e652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy:  0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"test_accuracy: \", nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1594920888822,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "p5S9qeCgsJSg",
    "outputId": "6339e65d-d66e-4c96-9053-ea70d0abdea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy:  0.7610155830198818\n"
     ]
    }
   ],
   "source": [
    "print(\"train_accuracy: \", nltk.classify.accuracy(classifier, train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSNI7OFxGib0"
   },
   "source": [
    "**Mejores atributos**\n",
    "\n",
    "¿Pero como podemos coger mejores atributos para intentar aumentar la precisión del modelo? Vamos a hacerlo por prueba y error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "k5uaIAdDGlq8"
   },
   "outputs": [],
   "source": [
    "def mas_atributos(nombre):\n",
    "    atrib = {}\n",
    "    atrib[\"primera_letra\"] = nombre[0].lower()\n",
    "    atrib[\"ultima_letra\"] = nombre[-1].lower()\n",
    "    for letra in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra)\n",
    "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower())\n",
    "    return atrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6-gJIxKcHKvI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primera_letra': 'j',\n",
       " 'ultima_letra': 'n',\n",
       " 'count(a)': 0,\n",
       " 'has(a)': False,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas_atributos('jhon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FBu25HHgHQtK"
   },
   "outputs": [],
   "source": [
    "fset = [(mas_atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 2127,
     "status": "ok",
     "timestamp": 1594920889465,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "8hWR9hOzHlNe",
    "outputId": "97d5f514-dfd5-474a-8755-4127a7bcb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy:  0.796\n"
     ]
    }
   ],
   "source": [
    "print(\"test_accuracy: \", nltk.classify.accuracy(classifier2, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio de práctica\n",
    "\n",
    "**Objetivo:** Construye un classificador de nombres en español usando el siguiente dataset: \n",
    "https://github.com/jvalhondo/spanish-names-surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Preparación de los datos**: con un `git clone` puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset `names` de nombres en ingles. \n",
    "\n",
    "* **Piensa y analiza**: ¿los features en ingles aplican de la misma manera para los nombres en español?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de los datos:\n",
    "\n",
    "import csv\n",
    "\n",
    "f_file, m_file = open('datasets/Spanish_Names/female_names.csv'), open('datasets/Spanish_Names/male_names.csv')\n",
    "f_reader, m_reader = csv.reader(f_file), csv.reader(m_file)\n",
    "\n",
    "# Ignoramos las cabeceras:\n",
    "next(f_reader)\n",
    "next(m_reader)\n",
    "\n",
    "tagset = ([(row[0], 'male') for row in m_reader] + [(row[0], 'female') for row in f_reader])\n",
    "random.shuffle(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ANDREW RICHARD', 'male'),\n",
       " ('NABI', 'male'),\n",
       " ('DORA LILIA', 'female'),\n",
       " ('RIE', 'female'),\n",
       " ('GALDER', 'male'),\n",
       " ('AITANA', 'female'),\n",
       " ('BLANCA SOLEDAD', 'female'),\n",
       " ('WEIMING', 'male'),\n",
       " ('PRADEEP', 'male'),\n",
       " ('RICARDO EMANUEL', 'male')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Entrenamiento y performance del modelo**: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3*. **Mejores atributos:** Define una función como `atributos2()` donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cogemos primero como atributo la última letra de un nombre español:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_1(name):\n",
    "    return {'last_letter': name[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.7893529893529894\n",
      "test_accuracy:  0.828\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_1(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_1('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_1('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_1('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_1('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_1('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_1('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_1('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_1('Jose María')))\n",
    "print(c1_esp.classify(atr_1('Carlos')))\n",
    "print(c1_esp.classify(atr_1('Javier')))\n",
    "print(c1_esp.classify(atr_1('Javier Rey')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a coger más atributos ahora:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_2(name):\n",
    "    atrs = {}\n",
    "    atrs[\"first_letter\"] = name[0].lower()\n",
    "    atrs[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8004095004095004\n",
      "test_accuracy:  0.804\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_2(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_2('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_2('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_2('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_2('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_2('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_2('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_2('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_2('Jose María')))\n",
    "print(c1_esp.classify(atr_2('Carlos')))\n",
    "print(c1_esp.classify(atr_2('Javier')))\n",
    "print(c1_esp.classify(atr_2('Javier Rey')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_3(name):\n",
    "    atrs = {}\n",
    "    name_surnames = name.split(' ')\n",
    "    \n",
    "    ## Getting first and last letter from name:\n",
    "    atrs[\"first_letter_name\"] = name_surnames[0][0].lower()\n",
    "    atrs[\"last_letter_name\"] = name_surnames[0][-1].lower()\n",
    "    \n",
    "    ## Getting first and last letter from first surname (if has):\n",
    "    if len(name_surnames) > 1:\n",
    "        atrs[\"first_letter_firstSurname\"] = name_surnames[1][0].lower()\n",
    "        atrs[\"last_letter_firstSurname\"] = name_surnames[1][-1].lower()\n",
    "    \n",
    "    ## Getting first and last letter from Second surname (if has):\n",
    "    if len(name_surnames) > 2:\n",
    "        atrs[\"first_letter_secondSurname\"] = name_surnames[2][0].lower()\n",
    "        atrs[\"last_letter_secondSurname\"] = name_surnames[2][-1].lower()\n",
    "    \n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letter_name': 's',\n",
       " 'last_letter_name': 't',\n",
       " 'first_letter_firstSurname': 'k',\n",
       " 'last_letter_firstSurname': 'r',\n",
       " 'first_letter_secondSurname': 'j',\n",
       " 'last_letter_secondSurname': 'i',\n",
       " 'count(a)': 2,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 1,\n",
       " 'has(e)': True,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 2,\n",
       " 'has(i)': True,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 1,\n",
       " 'has(k)': True,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 2,\n",
       " 'has(m)': True,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 0,\n",
       " 'has(o)': False,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 1,\n",
       " 'has(r)': True,\n",
       " 'count(s)': 1,\n",
       " 'has(s)': True,\n",
       " 'count(t)': 2,\n",
       " 'has(t)': True,\n",
       " 'count(u)': 2,\n",
       " 'has(u)': True,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atr_3(\"Sumit Kumar Jethani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8506756756756757\n",
      "test_accuracy:  0.842\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_3(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_3('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_3('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_3('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_3('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_3('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_3('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_3('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_3('Jose María')))\n",
    "print(c1_esp.classify(atr_3('Carlos')))\n",
    "print(c1_esp.classify(atr_3('Javier')))\n",
    "print(c1_esp.classify(atr_3('Javier Rey')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_4(name):\n",
    "    atrs = {}\n",
    "    name_surnames = name.split(' ')\n",
    "    \n",
    "    ## Getting the two first and last letters from name:\n",
    "    atrs[\"first_letters_name\"] = name_surnames[0][:2].lower()\n",
    "    atrs[\"last_letters_name\"] = name_surnames[0][-2:].lower()\n",
    "    \n",
    "    ## Getting the two first and last letters from first surname (if has):\n",
    "    if len(name_surnames) > 1:\n",
    "        atrs[\"first_letters_firstSurname\"] = name_surnames[1][:2].lower()\n",
    "        atrs[\"last_letters_firstSurname\"] = name_surnames[1][-2:].lower()\n",
    "    \n",
    "    ## Getting the two first and last letters from Second surname (if has):\n",
    "    if len(name_surnames) > 2:\n",
    "        atrs[\"first_letters_secondSurname\"] = name_surnames[2][:2].lower()\n",
    "        atrs[\"last_letters_secondSurname\"] = name_surnames[2][-2:].lower()\n",
    "    \n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letters_name': 'su',\n",
       " 'last_letters_name': 'mi',\n",
       " 'first_letters_firstSurname': 'ku',\n",
       " 'last_letters_firstSurname': 'ar',\n",
       " 'first_letters_secondSurname': 'je',\n",
       " 'last_letters_secondSurname': 'ni',\n",
       " 'count(a)': 2,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 1,\n",
       " 'has(e)': True,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 2,\n",
       " 'has(i)': True,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 1,\n",
       " 'has(k)': True,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 2,\n",
       " 'has(m)': True,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 0,\n",
       " 'has(o)': False,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 1,\n",
       " 'has(r)': True,\n",
       " 'count(s)': 1,\n",
       " 'has(s)': True,\n",
       " 'count(t)': 1,\n",
       " 'has(t)': True,\n",
       " 'count(u)': 2,\n",
       " 'has(u)': True,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atr_4(\"Sumi Kumar Jethani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8886363636363637\n",
      "test_accuracy:  0.872\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_4(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_4('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_4('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_4('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_4('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_4('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_4('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_4('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_4('Jose María')))\n",
    "print(c1_esp.classify(atr_4('Carlos')))\n",
    "print(c1_esp.classify(atr_4('Javier')))\n",
    "print(c1_esp.classify(atr_4('Laura')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos seguir asi probando distintos atributos y esto es conocido como: **íngeniería de atributos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7CXFyfoGf4s"
   },
   "source": [
    "# Clasificación de documentos (email spam o no spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 3628,
     "status": "ok",
     "timestamp": 1594920890983,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "Qfli08sgIzl_",
    "outputId": "7b634f01-0956-4843-8eb3-a4e5d2c77d3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'datasets'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pachocamacho1990/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 3626,
     "status": "ok",
     "timestamp": 1594920890985,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "bHFKXxclJ5LC",
    "outputId": "e2878b62-45ed-482a-faa2-92ce90b87731"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 4259,
     "status": "ok",
     "timestamp": 1594920891622,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "33oKcvcjKrlM",
    "outputId": "6183550d-66c9-41a4-e2a6-52c9da7ec461"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>contenido</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>[&lt;, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&gt; Russell Turpin:\\r\\n&gt; &gt; That depends on how t...</td>\n",
       "      <td>[&gt;, Russell, Turpin, :, &gt;, &gt;, That, depends, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Help wanted.  We are a 14 year old fortune 500...</td>\n",
       "      <td>[Help, wanted, ., We, are, a, 14, year, old, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Request A Free No Obligation Consultation!\\r\\n...</td>\n",
       "      <td>[Request, A, Free, No, Obligation, Consultatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Is there a way to look for a particular file o...</td>\n",
       "      <td>[Is, there, a, way, to, look, for, a, particul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clase                                          contenido  \\\n",
       "0     -1  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "1      1  > Russell Turpin:\\r\\n> > That depends on how t...   \n",
       "2     -1  Help wanted.  We are a 14 year old fortune 500...   \n",
       "3     -1  Request A Free No Obligation Consultation!\\r\\n...   \n",
       "4      1  Is there a way to look for a particular file o...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...  \n",
       "1  [>, Russell, Turpin, :, >, >, That, depends, o...  \n",
       "2  [Help, wanted, ., We, are, a, 14, year, old, f...  \n",
       "3  [Request, A, Free, No, Obligation, Consultatio...  \n",
       "4  [Is, there, a, way, to, look, for, a, particul...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/email/csv/spam-apache.csv', names = ['clase','contenido'])\n",
    "df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OvHkYDylNMKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " '!',\n",
       " 'DOCTYPE',\n",
       " 'HTML',\n",
       " 'PUBLIC',\n",
       " '``',\n",
       " '-//W3C//DTD',\n",
       " 'HTML',\n",
       " '4.0',\n",
       " 'Transitional//EN',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'HTML',\n",
       " '>',\n",
       " '<',\n",
       " 'HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'http-equiv=Content-Type',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'text/html',\n",
       " ';',\n",
       " 'charset=iso-8859-1',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'MSHTML',\n",
       " '6.00.2600.0',\n",
       " \"''\",\n",
       " 'name=GENERATOR',\n",
       " '>',\n",
       " '<',\n",
       " 'STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'BODY',\n",
       " 'bgColor=',\n",
       " '#',\n",
       " 'ffffff',\n",
       " '>',\n",
       " '<',\n",
       " 'DIV',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=Arial',\n",
       " 'size=2',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=',\n",
       " \"''\",\n",
       " 'Times',\n",
       " 'New',\n",
       " 'Roman',\n",
       " \"''\",\n",
       " 'size=3',\n",
       " '>',\n",
       " 'Dear',\n",
       " 'Friend',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'A',\n",
       " 'recent',\n",
       " 'survey',\n",
       " 'by',\n",
       " 'Nielsen/Netratings',\n",
       " 'says',\n",
       " 'that',\n",
       " '``',\n",
       " 'The',\n",
       " 'Internet',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'population',\n",
       " 'is',\n",
       " 'rapidly',\n",
       " 'approaching',\n",
       " 'a',\n",
       " \"'Half\",\n",
       " 'a',\n",
       " 'Billion',\n",
       " \"'\",\n",
       " 'people',\n",
       " '!',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'SO',\n",
       " 'WHAT',\n",
       " 'DOES',\n",
       " 'ALL',\n",
       " 'THIS',\n",
       " 'MEAN',\n",
       " 'TO',\n",
       " 'YOU',\n",
       " '?',\n",
       " 'EASY',\n",
       " 'MONEY',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'assume',\n",
       " 'that',\n",
       " 'every',\n",
       " 'person',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'E-mail',\n",
       " 'address',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'that',\n",
       " \"'s\",\n",
       " '500',\n",
       " 'million',\n",
       " 'potential',\n",
       " 'customers',\n",
       " 'and',\n",
       " 'growing',\n",
       " '!',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"E'mail\",\n",
       " 'is',\n",
       " 'without',\n",
       " 'question',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'method',\n",
       " 'of',\n",
       " 'distributing',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'I',\n",
       " 'think',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'picture',\n",
       " '.',\n",
       " 'The',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'potential',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'just',\n",
       " 'staggering',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'even',\n",
       " 'better',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Suppose',\n",
       " 'I',\n",
       " 'told',\n",
       " 'you',\n",
       " 'that',\n",
       " 'you',\n",
       " 'could',\n",
       " 'start',\n",
       " 'your',\n",
       " 'own',\n",
       " 'E-mail',\n",
       " 'business',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'today',\n",
       " 'and',\n",
       " 'enjoy',\n",
       " 'these',\n",
       " 'benefits',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'All',\n",
       " 'Customers',\n",
       " 'Pay',\n",
       " 'You',\n",
       " 'In',\n",
       " 'Cash',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Will',\n",
       " 'Sell',\n",
       " 'A',\n",
       " 'Product',\n",
       " 'Which',\n",
       " 'Costs',\n",
       " 'Nothing',\n",
       " 'to',\n",
       " 'Produce',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'Your',\n",
       " 'Only',\n",
       " 'Overhead',\n",
       " 'Is',\n",
       " 'Your',\n",
       " 'Time',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Have',\n",
       " '100s',\n",
       " 'Of',\n",
       " 'Millions',\n",
       " 'Of',\n",
       " 'Potential',\n",
       " 'Customers',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Get',\n",
       " 'Detailed',\n",
       " ',',\n",
       " 'Easy',\n",
       " 'To',\n",
       " 'Follow',\n",
       " 'Startup',\n",
       " 'Instructions',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THIS',\n",
       " 'IS',\n",
       " 'JUST',\n",
       " 'THE',\n",
       " 'TIP',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'ICEBERG',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'As',\n",
       " 'you',\n",
       " 'read',\n",
       " 'on',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'discover',\n",
       " 'how',\n",
       " 'a',\n",
       " \"'Seen\",\n",
       " 'on',\n",
       " 'National',\n",
       " 'TV',\n",
       " \"'\",\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'is',\n",
       " 'paying',\n",
       " 'out',\n",
       " 'a',\n",
       " 'half',\n",
       " 'million',\n",
       " 'dollars',\n",
       " ',',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " 'months',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'from',\n",
       " 'your',\n",
       " 'home',\n",
       " ',',\n",
       " 'for',\n",
       " 'an',\n",
       " 'investment',\n",
       " 'of',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'US',\n",
       " 'Dollars',\n",
       " 'expense',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'one',\n",
       " 'time',\n",
       " '.',\n",
       " 'ALL',\n",
       " 'THANKS',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'COMPUTER',\n",
       " 'AGE',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THE',\n",
       " 'INTERNET',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Before',\n",
       " 'you',\n",
       " 'say',\n",
       " '``',\n",
       " 'Bull',\n",
       " \"''\",\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'you',\n",
       " 'have',\n",
       " 'been',\n",
       " 'hearing',\n",
       " 'about',\n",
       " 'on',\n",
       " 'the',\n",
       " 'news',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'lately',\n",
       " '.',\n",
       " 'Due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'this',\n",
       " 'letter',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'a',\n",
       " 'national',\n",
       " 'weekly',\n",
       " 'news',\n",
       " 'program',\n",
       " 'recently',\n",
       " 'devoted',\n",
       " 'an',\n",
       " 'entire',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'show',\n",
       " 'to',\n",
       " 'the',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'this',\n",
       " 'program',\n",
       " 'described',\n",
       " 'below',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'to',\n",
       " 'see',\n",
       " 'if',\n",
       " 'it',\n",
       " 'really',\n",
       " 'can',\n",
       " 'make',\n",
       " 'people',\n",
       " 'money.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'show',\n",
       " 'also',\n",
       " 'investigated',\n",
       " 'whether',\n",
       " 'or',\n",
       " 'not',\n",
       " 'the',\n",
       " 'program',\n",
       " 'was',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'legal',\n",
       " '.',\n",
       " 'Their',\n",
       " 'findings',\n",
       " 'proved',\n",
       " 'once',\n",
       " 'and',\n",
       " 'for',\n",
       " 'all',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'absolutely',\n",
       " 'NO',\n",
       " 'laws',\n",
       " 'prohibiting',\n",
       " 'the',\n",
       " 'participation',\n",
       " 'in',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'and',\n",
       " 'if',\n",
       " 'people',\n",
       " 'can',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'they',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'make',\n",
       " 'some',\n",
       " 'mega',\n",
       " 'bucks',\n",
       " 'with',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'out',\n",
       " 'of',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'pocket',\n",
       " 'cost',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'DUE',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'RECENT',\n",
       " 'INCREASE',\n",
       " 'OF',\n",
       " 'POPULARITY',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'RESPECT',\n",
       " 'THIS',\n",
       " 'PROGRAM',\n",
       " 'HAS',\n",
       " 'ATTAINED',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'IT',\n",
       " 'IS',\n",
       " 'CURRENTLY',\n",
       " 'WORKING',\n",
       " 'BETTER',\n",
       " 'THAN',\n",
       " 'EVER',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'This',\n",
       " 'is',\n",
       " 'what',\n",
       " 'one',\n",
       " 'had',\n",
       " 'to',\n",
       " 'say',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'Thanks',\n",
       " 'to',\n",
       " 'this',\n",
       " 'profitable',\n",
       " 'opportunity',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'approached',\n",
       " 'many',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'times',\n",
       " 'before',\n",
       " 'but',\n",
       " 'each',\n",
       " 'time',\n",
       " 'I',\n",
       " 'passed',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'so',\n",
       " 'glad',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'finally',\n",
       " 'joined',\n",
       " 'just',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'one',\n",
       " 'could',\n",
       " 'expect',\n",
       " 'in',\n",
       " 'return',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'for',\n",
       " 'the',\n",
       " 'minimal',\n",
       " 'effort',\n",
       " 'and',\n",
       " 'money',\n",
       " 'required',\n",
       " '.',\n",
       " 'To',\n",
       " 'my',\n",
       " 'asonishment',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'I',\n",
       " 'received',\n",
       " 'total',\n",
       " '$',\n",
       " '610,470.00',\n",
       " 'in',\n",
       " '21',\n",
       " 'weeks',\n",
       " ',',\n",
       " 'with',\n",
       " 'money',\n",
       " 'still',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'coming',\n",
       " 'in',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Pam',\n",
       " 'Hedland',\n",
       " ',',\n",
       " 'Fort',\n",
       " 'Lee',\n",
       " ',',\n",
       " 'New',\n",
       " 'Jersey',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'another',\n",
       " 'testimonial',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'This',\n",
       " 'program',\n",
       " 'has',\n",
       " 'been',\n",
       " 'around',\n",
       " 'for',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'but',\n",
       " 'I',\n",
       " 'never',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'believed',\n",
       " 'in',\n",
       " 'it',\n",
       " '.',\n",
       " 'But',\n",
       " 'one',\n",
       " 'day',\n",
       " 'when',\n",
       " 'I',\n",
       " 'received',\n",
       " 'this',\n",
       " 'again',\n",
       " 'in',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'the',\n",
       " 'mail',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'gamble',\n",
       " 'my',\n",
       " '$',\n",
       " '25',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'followed',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " 'and',\n",
       " 'walaa',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'later',\n",
       " 'the',\n",
       " 'money',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'started',\n",
       " 'to',\n",
       " 'come',\n",
       " 'in',\n",
       " '.',\n",
       " 'First',\n",
       " 'month',\n",
       " 'I',\n",
       " 'only',\n",
       " 'made',\n",
       " '$',\n",
       " '240.00',\n",
       " 'but',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'next',\n",
       " '2',\n",
       " 'months',\n",
       " 'after',\n",
       " 'that',\n",
       " 'I',\n",
       " 'made',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '$',\n",
       " '290,000.00.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'So',\n",
       " 'far',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '8',\n",
       " 'months',\n",
       " 'by',\n",
       " 're-entering',\n",
       " 'the',\n",
       " 'program',\n",
       " ',',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'have',\n",
       " 'made',\n",
       " 'over',\n",
       " '$',\n",
       " '710,000.00',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'playing',\n",
       " 'it',\n",
       " 'again.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'key',\n",
       " 'to',\n",
       " 'success',\n",
       " 'in',\n",
       " 'this',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'steps',\n",
       " 'and',\n",
       " 'NOT',\n",
       " 'change',\n",
       " 'anything',\n",
       " '.',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'More',\n",
       " 'testimonials',\n",
       " 'later',\n",
       " 'but',\n",
       " 'first',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " 'PRINT',\n",
       " 'THIS',\n",
       " 'NOW',\n",
       " 'FOR',\n",
       " 'YOUR',\n",
       " 'FUTURE',\n",
       " 'REFERENCE',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'If',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'make',\n",
       " 'at',\n",
       " 'least',\n",
       " '$',\n",
       " '500,000',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'months',\n",
       " 'easily',\n",
       " 'and',\n",
       " 'comfortably',\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'THEN',\n",
       " 'READ',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O4kw1BQUOe4-"
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist])\n",
    "top_words = all_words.most_common(200)\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1g6F_qNfmRAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"contains((',', 2173))\": False,\n",
       " \"contains(('.', 2171))\": False,\n",
       " \"contains(('the', 1967))\": False,\n",
       " \"contains(('>', 1787))\": False,\n",
       " \"contains(('--', 1611))\": False,\n",
       " \"contains(('to', 1435))\": False,\n",
       " \"contains((':', 1220))\": False,\n",
       " \"contains(('*', 1149))\": False,\n",
       " \"contains(('and', 1064))\": False,\n",
       " \"contains(('of', 958))\": False,\n",
       " \"contains(('a', 879))\": False,\n",
       " \"contains(('you', 744))\": False,\n",
       " \"contains(('in', 742))\": False,\n",
       " \"contains(('I', 741))\": False,\n",
       " \"contains(('<', 718))\": False,\n",
       " \"contains(('!', 698))\": False,\n",
       " \"contains(('%', 677))\": False,\n",
       " \"contains(('for', 609))\": False,\n",
       " \"contains(('is', 578))\": False,\n",
       " \"contains(('#', 521))\": False,\n",
       " \"contains(('BR', 494))\": False,\n",
       " \"contains(('that', 479))\": False,\n",
       " \"contains((')', 463))\": False,\n",
       " \"contains(('it', 458))\": False,\n",
       " 'contains((\"\\'\\'\", 434))': False,\n",
       " \"contains(('$', 413))\": False,\n",
       " \"contains(('this', 384))\": False,\n",
       " \"contains(('(', 380))\": False,\n",
       " \"contains(('on', 378))\": False,\n",
       " \"contains(('http', 362))\": False,\n",
       " \"contains(('?', 360))\": False,\n",
       " \"contains(('your', 359))\": False,\n",
       " \"contains(('have', 351))\": False,\n",
       " \"contains(('with', 334))\": False,\n",
       " \"contains(('``', 307))\": False,\n",
       " \"contains(('be', 299))\": False,\n",
       " \"contains(('-', 289))\": False,\n",
       " \"contains(('from', 271))\": False,\n",
       " 'contains((\"\\'s\", 263))': False,\n",
       " \"contains(('are', 257))\": False,\n",
       " \"contains(('31', 255))\": False,\n",
       " \"contains(('or', 252))\": False,\n",
       " \"contains(('as', 251))\": False,\n",
       " \"contains(('will', 243))\": False,\n",
       " \"contains(('not', 226))\": False,\n",
       " \"contains(('30', 220))\": False,\n",
       " \"contains(('my', 206))\": False,\n",
       " \"contains(('at', 199))\": False,\n",
       " \"contains(('The', 198))\": False,\n",
       " \"contains(('has', 195))\": False,\n",
       " \"contains(('can', 194))\": False,\n",
       " \"contains(('&', 181))\": False,\n",
       " \"contains(('all', 176))\": False,\n",
       " 'contains((\"n\\'t\", 175))': False,\n",
       " \"contains(('do', 167))\": False,\n",
       " \"contains(('out', 166))\": False,\n",
       " \"contains(('but', 164))\": False,\n",
       " \"contains(('...', 160))\": False,\n",
       " \"contains(('our', 160))\": False,\n",
       " \"contains(('by', 156))\": False,\n",
       " \"contains(('if', 152))\": False,\n",
       " \"contains(('was', 149))\": False,\n",
       " \"contains(('one', 129))\": False,\n",
       " \"contains(('an', 129))\": False,\n",
       " \"contains(('just', 128))\": False,\n",
       " \"contains(('@', 128))\": False,\n",
       " \"contains(('This', 125))\": False,\n",
       " \"contains(('1', 123))\": False,\n",
       " \"contains(('If', 118))\": False,\n",
       " \"contains(('more', 118))\": False,\n",
       " \"contains(('You', 117))\": False,\n",
       " \"contains(('5', 116))\": False,\n",
       " \"contains(('we', 116))\": False,\n",
       " \"contains(('time', 114))\": False,\n",
       " \"contains(('people', 110))\": False,\n",
       " \"contains(('me', 110))\": False,\n",
       " \"contains(('We', 110))\": False,\n",
       " \"contains(('THE', 108))\": False,\n",
       " \"contains(('up', 108))\": False,\n",
       " \"contains(('get', 107))\": False,\n",
       " \"contains(('they', 103))\": False,\n",
       " \"contains(('only', 100))\": False,\n",
       " \"contains(('like', 100))\": False,\n",
       " \"contains(('so', 99))\": False,\n",
       " 'contains((\"\\'\", 95))': False,\n",
       " \"contains(('To', 95))\": False,\n",
       " \"contains(('list', 95))\": False,\n",
       " \"contains(('2', 94))\": False,\n",
       " \"contains(('other', 92))\": False,\n",
       " \"contains(('A', 91))\": False,\n",
       " \"contains(('FREE', 90))\": False,\n",
       " \"contains(('No', 90))\": False,\n",
       " \"contains(('would', 88))\": False,\n",
       " \"contains(('any', 88))\": False,\n",
       " \"contains(('been', 87))\": False,\n",
       " \"contains(('who', 87))\": False,\n",
       " \"contains(('there', 86))\": False,\n",
       " \"contains(('which', 86))\": False,\n",
       " \"contains(('|', 84))\": False,\n",
       " \"contains(('about', 81))\": False,\n",
       " \"contains((']', 81))\": False,\n",
       " \"contains(('some', 80))\": False,\n",
       " \"contains(('email', 80))\": False,\n",
       " \"contains(('what', 79))\": False,\n",
       " \"contains(('AND', 77))\": False,\n",
       " \"contains(('their', 76))\": False,\n",
       " \"contains(('TO', 75))\": False,\n",
       " \"contains(('no', 75))\": False,\n",
       " \"contains(('then', 74))\": False,\n",
       " \"contains(('his', 74))\": False,\n",
       " \"contains(('It', 74))\": False,\n",
       " \"contains(('address', 73))\": False,\n",
       " \"contains(('use', 73))\": False,\n",
       " \"contains(('YOU', 72))\": False,\n",
       " \"contains(('money', 72))\": False,\n",
       " \"contains(('3', 72))\": False,\n",
       " \"contains(('[', 71))\": False,\n",
       " \"contains(('each', 70))\": False,\n",
       " \"contains(('work', 70))\": False,\n",
       " \"contains(('over', 69))\": False,\n",
       " \"contains(('he', 69))\": False,\n",
       " \"contains(('make', 68))\": False,\n",
       " \"contains(('send', 68))\": False,\n",
       " \"contains(('them', 68))\": False,\n",
       " \"contains(('OF', 67))\": False,\n",
       " \"contains(('name', 67))\": False,\n",
       " \"contains(('than', 67))\": False,\n",
       " \"contains(('2002', 67))\": False,\n",
       " \"contains(('could', 66))\": False,\n",
       " \"contains(('am', 66))\": False,\n",
       " \"contains(('unseen', 65))\": False,\n",
       " \"contains(('see', 63))\": False,\n",
       " \"contains(('YOUR', 63))\": False,\n",
       " \"contains(('4', 61))\": False,\n",
       " \"contains(('how', 60))\": False,\n",
       " \"contains(('way', 60))\": False,\n",
       " \"contains(('msgs', 59))\": False,\n",
       " \"contains(('lists/l-k', 59))\": False,\n",
       " \"contains(('wrote', 58))\": False,\n",
       " \"contains(('also', 57))\": False,\n",
       " \"contains(('here', 57))\": False,\n",
       " \"contains(('Your', 56))\": False,\n",
       " \"contains(('mail', 56))\": False,\n",
       " \"contains(('receive', 56))\": False,\n",
       " \"contains(('go', 56))\": False,\n",
       " \"contains(('program', 55))\": False,\n",
       " \"contains(('On', 55))\": False,\n",
       " \"contains(('new', 55))\": False,\n",
       " \"contains(('had', 54))\": False,\n",
       " \"contains(('NOT', 54))\": False,\n",
       " \"contains(('does', 54))\": False,\n",
       " \"contains(('want', 54))\": False,\n",
       " \"contains(('please', 53))\": False,\n",
       " \"contains(('us', 53))\": False,\n",
       " \"contains(('because', 53))\": False,\n",
       " \"contains(('REPORT', 52))\": False,\n",
       " \"contains(('below', 51))\": False,\n",
       " \"contains(('when', 51))\": False,\n",
       " \"contains(('e-mail', 51))\": False,\n",
       " 'contains((\"\\'m\", 51))': False,\n",
       " \"contains(('free', 50))\": False,\n",
       " \"contains(('think', 49))\": False,\n",
       " \"contains(('now', 49))\": False,\n",
       " \"contains(('first', 48))\": False,\n",
       " \"contains(('Please', 48))\": False,\n",
       " \"contains(('most', 47))\": False,\n",
       " \"contains(('within', 47))\": False,\n",
       " \"contains(('even', 46))\": False,\n",
       " \"contains(('using', 46))\": False,\n",
       " \"contains(('e-mails', 45))\": False,\n",
       " \"contains(('sent', 45))\": False,\n",
       " \"contains(('THIS', 44))\": False,\n",
       " \"contains(('In', 44))\": False,\n",
       " \"contains(('received', 44))\": False,\n",
       " \"contains(('company', 44))\": False,\n",
       " \"contains(('need', 43))\": False,\n",
       " \"contains(('much', 43))\": False,\n",
       " \"contains(('nbsp=3B', 43))\": False,\n",
       " \"contains(('did', 42))\": False,\n",
       " \"contains(('And', 42))\": False,\n",
       " \"contains(('still', 41))\": False,\n",
       " \"contains(('FOR', 41))\": False,\n",
       " 'contains((\"\\'re\", 41))': False,\n",
       " \"contains(('know', 41))\": False,\n",
       " \"contains((';', 40))\": False,\n",
       " \"contains(('=', 40))\": False,\n",
       " \"contains(('days', 40))\": False,\n",
       " \"contains(('where', 40))\": False,\n",
       " \"contains(('//www.adclick.ws/p.cfm', 40))\": False,\n",
       " \"contains(('line', 40))\": False,\n",
       " \"contains(('information', 39))\": False,\n",
       " \"contains(('US', 39))\": False,\n",
       " \"contains(('through', 39))\": False,\n",
       " \"contains(('message', 39))\": False,\n",
       " \"contains(('Linux', 39))\": False,\n",
       " 'contains((\"\\'ve\", 39))': False,\n",
       " \"contains(('made', 38))\": False,\n",
       " \"contains(('different', 38))\": False,\n",
       " \"contains(('those', 38))\": False,\n",
       " \"contains(('Report', 38))\": False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_features(df['tokens'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SrCXXGMCn3zz"
   },
   "outputs": [],
   "source": [
    "fset = [(document_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
    "random.shuffle(fset)\n",
    "train, test = fset[:200], fset[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r6FGZE4OqkEa"
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1594921479920,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "xIyVc6lBrGOy",
    "outputId": "01479a1e-6f68-4423-d4d3-6b1f9673f43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 4539,
     "status": "ok",
     "timestamp": 1594920891932,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "1x-R_PImrKIV",
    "outputId": "3b384c67-1640-422f-d9a1-19e41b42c667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "     contains((\"'\", 95)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"''\", 434)) = False              -1 : 1      =      1.0 : 1.0\n",
      "    contains((\"'m\", 51)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'re\", 41)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'s\", 263)) = False              -1 : 1      =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1594921643207,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "wKzgha92up3l",
    "outputId": "ec079b59-5973-4084-e6ab-2f66037e92e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...\n",
       "2      Help wanted.  We are a 14 year old fortune 500...\n",
       "3      Request A Free No Obligation Consultation!\\r\\n...\n",
       "10     >\\r\\n>“µ×è¹µÑÇ ¡ÑºâÅ¡¸ØÃ¡Ô¨º¹ÍÔ¹àµÍÃìà¹çµ” \\r\\...\n",
       "11     ==============================================...\n",
       "                             ...                        \n",
       "243    ##############################################...\n",
       "244    Wanna see sexually curious teens playing with ...\n",
       "246    REQUEST FOR URGENT BUSINESS ASSISTANCE\\r\\n----...\n",
       "248    Email marketing works!  There's no way around ...\n",
       "249    Email marketing works!  There's no way around ...\n",
       "Name: contenido, Length: 125, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['clase']==-1]['contenido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBvifrnr3GY"
   },
   "source": [
    "## Ejercicio de práctica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR53vedlvd1O"
   },
   "source": [
    "¿Como podrías construir un mejor clasificador de documentos?\n",
    "\n",
    "0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/` \n",
    "\n",
    "1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... . \n",
    "\n",
    "---\n",
    "\n",
    "Con base en eso construye un dataset más grande y con un tokenizado más pulido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "TOw2KrtnymVT"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construcción del dataset en un dataframe: usamos el corpus de correos spam y ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "v2ZO0aJyrTLx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "clases = []\n",
    "#lectura de spam data\n",
    "for file in os.listdir('datasets/email/plaintext/corpus1/spam'):\n",
    "  with open('datasets/email/plaintext/corpus1/spam/'+file, encoding='latin-1') as f:\n",
    "    data.append(f.read())\n",
    "    clases.append('spam')\n",
    "#lectura de ham data\n",
    "for file in os.listdir('datasets/email/plaintext/corpus1/ham'):\n",
    "  with open('datasets/email/plaintext/corpus1/ham/'+file, encoding='latin-1') as f:\n",
    "    data.append(f.read())\n",
    "    clases.append('ham')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>contenido</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: dobmeos with hgh my energy level has ...</td>\n",
       "      <td>[Subject, :, dobmeos, with, hgh, my, energy, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: your prescription is ready . . oxwq s...</td>\n",
       "      <td>[Subject, :, your, prescription, is, ready, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: get that new car 8434\\npeople nowthe ...</td>\n",
       "      <td>[Subject, :, get, that, new, car, 8434, people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: await your response\\ndear partner ,\\n...</td>\n",
       "      <td>[Subject, :, await, your, response, dear, part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: coca cola , mbna america , nascar par...</td>\n",
       "      <td>[Subject, :, coca, cola, ,, mbna, america, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : tenaska iv\\ni ' ll call you on t...</td>\n",
       "      <td>[Subject, :, re, :, tenaska, iv, i, ', ll, cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: generic contract\\nhi daren ,\\nsorry f...</td>\n",
       "      <td>[Subject, :, generic, contract, hi, daren, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : contracts and credit\\nthanks - -...</td>\n",
       "      <td>[Subject, :, re, :, contracts, and, credit, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : tenaska iv\\nok , since we don ' ...</td>\n",
       "      <td>[Subject, :, re, :, tenaska, iv, ok, ,, since,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : tenaska iv\\ni tried calling you ...</td>\n",
       "      <td>[Subject, :, re, :, tenaska, iv, i, tried, cal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5172 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     clase                                          contenido  \\\n",
       "0     spam  Subject: dobmeos with hgh my energy level has ...   \n",
       "1     spam  Subject: your prescription is ready . . oxwq s...   \n",
       "2     spam  Subject: get that new car 8434\\npeople nowthe ...   \n",
       "3     spam  Subject: await your response\\ndear partner ,\\n...   \n",
       "4     spam  Subject: coca cola , mbna america , nascar par...   \n",
       "...    ...                                                ...   \n",
       "5167   ham  Subject: re : tenaska iv\\ni ' ll call you on t...   \n",
       "5168   ham  Subject: generic contract\\nhi daren ,\\nsorry f...   \n",
       "5169   ham  Subject: re : contracts and credit\\nthanks - -...   \n",
       "5170   ham  Subject: re : tenaska iv\\nok , since we don ' ...   \n",
       "5171   ham  Subject: re : tenaska iv\\ni tried calling you ...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [Subject, :, dobmeos, with, hgh, my, energy, l...  \n",
       "1     [Subject, :, your, prescription, is, ready, .,...  \n",
       "2     [Subject, :, get, that, new, car, 8434, people...  \n",
       "3     [Subject, :, await, your, response, dear, part...  \n",
       "4     [Subject, :, coca, cola, ,, mbna, america, ,, ...  \n",
       "...                                                 ...  \n",
       "5167  [Subject, :, re, :, tenaska, iv, i, ', ll, cal...  \n",
       "5168  [Subject, :, generic, contract, hi, daren, ,, ...  \n",
       "5169  [Subject, :, re, :, contracts, and, credit, th...  \n",
       "5170  [Subject, :, re, :, tenaska, iv, ok, ,, since,...  \n",
       "5171  [Subject, :, re, :, tenaska, iv, i, tried, cal...  \n",
       "\n",
       "[5172 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Construcción del dataframe:\n",
    "\n",
    "df = pd.DataFrame(list(zip(clases,data)), columns = ['clase','contenido'])\n",
    "df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist])\n",
    "stopwd = stopwords.words('english')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a eliminar los siguientes tipos de tokens:\n",
    "- Aquellos que tengan una longitud < 4\n",
    "- Ademas aparezcan en all_words < 10\n",
    "- Formen parte de stopwords del idioma ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subject', 6676),\n",
       " ('please', 1895),\n",
       " ('2000', 1550),\n",
       " ('enron', 1462),\n",
       " ('thanks', 1432),\n",
       " ('know', 1112),\n",
       " ('daren', 1030),\n",
       " ('deal', 961),\n",
       " ('need', 921),\n",
       " ('forwarded', 909),\n",
       " ('attached', 886),\n",
       " ('2001', 801),\n",
       " ('corp', 789),\n",
       " ('meter', 773),\n",
       " ('would', 695),\n",
       " ('time', 663),\n",
       " ('questions', 652),\n",
       " ('information', 645),\n",
       " ('farmer', 630),\n",
       " ('http', 610),\n",
       " ('month', 596),\n",
       " ('also', 588),\n",
       " ('file', 545),\n",
       " ('call', 537),\n",
       " ('price', 536),\n",
       " ('like', 529),\n",
       " ('mmbtu', 527),\n",
       " ('volume', 523),\n",
       " ('message', 516),\n",
       " ('sent', 516),\n",
       " ('email', 504),\n",
       " ('want', 501),\n",
       " ('following', 487),\n",
       " ('change', 457),\n",
       " ('back', 457),\n",
       " ('contract', 457),\n",
       " ('mail', 445),\n",
       " ('today', 437),\n",
       " ('volumes', 437),\n",
       " ('texas', 433),\n",
       " ('make', 430),\n",
       " ('could', 420),\n",
       " ('flow', 415),\n",
       " ('deals', 411),\n",
       " ('sitara', 405),\n",
       " ('help', 380),\n",
       " ('energy', 376),\n",
       " ('take', 376),\n",
       " ('robert', 367),\n",
       " ('thank', 364),\n",
       " ('number', 361),\n",
       " ('contact', 359),\n",
       " ('free', 356),\n",
       " ('effective', 356),\n",
       " ('first', 352),\n",
       " ('daily', 350),\n",
       " ('production', 347),\n",
       " ('system', 347),\n",
       " ('list', 337),\n",
       " ('sale', 337),\n",
       " ('nomination', 335),\n",
       " ('original', 331),\n",
       " ('company', 330),\n",
       " ('best', 328),\n",
       " ('last', 328),\n",
       " ('send', 323),\n",
       " ('days', 321),\n",
       " ('ticket', 315),\n",
       " ('forward', 312),\n",
       " ('click', 311),\n",
       " ('next', 311),\n",
       " ('available', 309),\n",
       " ('purchase', 309),\n",
       " ('week', 309),\n",
       " ('well', 307),\n",
       " ('mary', 302),\n",
       " ('look', 301),\n",
       " ('north', 301),\n",
       " ('april', 298),\n",
       " ('work', 291),\n",
       " ('america', 284),\n",
       " ('give', 283),\n",
       " ('march', 282),\n",
       " ('business', 277),\n",
       " ('needs', 276),\n",
       " ('actuals', 276),\n",
       " ('made', 275),\n",
       " ('think', 273),\n",
       " ('line', 271),\n",
       " ('houston', 270),\n",
       " ('order', 268),\n",
       " ('january', 264),\n",
       " ('find', 263),\n",
       " ('date', 260),\n",
       " ('based', 259),\n",
       " ('office', 257),\n",
       " ('good', 256),\n",
       " ('total', 255),\n",
       " ('melissa', 255),\n",
       " ('sure', 254),\n",
       " ('receive', 254),\n",
       " ('july', 254),\n",
       " ('check', 252),\n",
       " ('teco', 251),\n",
       " ('currently', 250),\n",
       " ('june', 250),\n",
       " ('still', 250),\n",
       " ('friday', 250),\n",
       " ('plant', 249),\n",
       " ('name', 248),\n",
       " ('online', 246),\n",
       " ('gary', 244),\n",
       " ('group', 244),\n",
       " ('going', 243),\n",
       " ('high', 239),\n",
       " ('management', 239),\n",
       " ('noms', 239),\n",
       " ('agreement', 237),\n",
       " ('money', 237),\n",
       " ('less', 236),\n",
       " ('since', 235),\n",
       " ('year', 233),\n",
       " ('phone', 232),\n",
       " ('changes', 232),\n",
       " ('report', 231),\n",
       " ('received', 231),\n",
       " ('note', 230),\n",
       " ('david', 228),\n",
       " ('michael', 228),\n",
       " ('keep', 225),\n",
       " ('smith', 224),\n",
       " ('without', 219),\n",
       " ('sales', 219),\n",
       " ('delivery', 219),\n",
       " ('monday', 218),\n",
       " ('many', 215),\n",
       " ('looking', 215),\n",
       " ('service', 215),\n",
       " ('point', 215),\n",
       " ('mark', 214),\n",
       " ('able', 212),\n",
       " ('address', 212),\n",
       " ('vance', 212),\n",
       " ('place', 211),\n",
       " ('additional', 210),\n",
       " ('future', 210),\n",
       " ('show', 209),\n",
       " ('much', 208),\n",
       " ('people', 208),\n",
       " ('copy', 207),\n",
       " ('request', 206),\n",
       " ('supply', 205),\n",
       " ('current', 204),\n",
       " ('long', 203),\n",
       " ('desk', 203),\n",
       " ('process', 202),\n",
       " ('offer', 202),\n",
       " ('prices', 202),\n",
       " ('link', 202),\n",
       " ('october', 202),\n",
       " ('services', 200),\n",
       " ('within', 199),\n",
       " ('right', 198),\n",
       " ('august', 198),\n",
       " ('possible', 196),\n",
       " ('julie', 196),\n",
       " ('visit', 195),\n",
       " ('great', 195),\n",
       " ('done', 194),\n",
       " ('info', 194),\n",
       " ('special', 193),\n",
       " ('pipeline', 192),\n",
       " ('however', 192),\n",
       " ('taylor', 191),\n",
       " ('come', 190),\n",
       " ('provide', 190),\n",
       " ('transport', 190),\n",
       " ('resources', 186),\n",
       " ('regarding', 184),\n",
       " ('chokshi', 184),\n",
       " ('clynes', 184),\n",
       " ('even', 183),\n",
       " ('prior', 183),\n",
       " ('home', 183),\n",
       " ('december', 183),\n",
       " ('correct', 183),\n",
       " ('actual', 182),\n",
       " ('every', 182),\n",
       " ('counterparty', 182),\n",
       " ('needed', 181),\n",
       " ('said', 181),\n",
       " ('products', 180),\n",
       " ('lisa', 180),\n",
       " ('stop', 178),\n",
       " ('susan', 177),\n",
       " ('team', 176),\n",
       " ('steve', 176),\n",
       " ('soon', 176),\n",
       " ('issue', 176),\n",
       " ('believe', 175),\n",
       " ('part', 175),\n",
       " ('anything', 175),\n",
       " ('using', 174),\n",
       " ('thursday', 174),\n",
       " ('either', 174),\n",
       " ('must', 173),\n",
       " ('reply', 173),\n",
       " ('review', 173),\n",
       " ('someone', 172),\n",
       " ('months', 171),\n",
       " ('february', 170),\n",
       " ('numbers', 169),\n",
       " ('tuesday', 169),\n",
       " ('george', 169),\n",
       " ('feel', 168),\n",
       " ('notice', 167),\n",
       " ('details', 166),\n",
       " ('site', 166),\n",
       " ('entered', 166),\n",
       " ('term', 165),\n",
       " ('september', 164),\n",
       " ('create', 163),\n",
       " ('increase', 163),\n",
       " ('start', 163),\n",
       " ('november', 163),\n",
       " ('another', 161),\n",
       " ('include', 161),\n",
       " ('used', 160),\n",
       " ('save', 160),\n",
       " ('created', 160),\n",
       " ('world', 158),\n",
       " ('getting', 158),\n",
       " ('already', 158),\n",
       " ('rate', 158),\n",
       " ('control', 158),\n",
       " ('update', 158),\n",
       " ('meeting', 158),\n",
       " ('follow', 157),\n",
       " ('cotten', 157),\n",
       " ('hours', 156),\n",
       " ('access', 156),\n",
       " ('customer', 156),\n",
       " ('john', 153),\n",
       " ('fuel', 153),\n",
       " ('agree', 152),\n",
       " ('read', 151),\n",
       " ('paid', 151),\n",
       " ('duke', 151),\n",
       " ('continue', 151),\n",
       " ('wellhead', 151),\n",
       " ('market', 150),\n",
       " ('regards', 150),\n",
       " ('works', 150),\n",
       " ('problem', 150),\n",
       " ('product', 149),\n",
       " ('meters', 149),\n",
       " ('period', 149),\n",
       " ('contracts', 148),\n",
       " ('manager', 148),\n",
       " ('activity', 148),\n",
       " ('advise', 147),\n",
       " ('trading', 147),\n",
       " ('better', 146),\n",
       " ('account', 146),\n",
       " ('morning', 146),\n",
       " ('revised', 146),\n",
       " ('jones', 146),\n",
       " ('trade', 146),\n",
       " ('graves', 146),\n",
       " ('megan', 146),\n",
       " ('tenaska', 146),\n",
       " ('natural', 145),\n",
       " ('working', 145),\n",
       " ('james', 144),\n",
       " ('might', 144),\n",
       " ('meyers', 143),\n",
       " ('internet', 142),\n",
       " ('support', 142),\n",
       " ('little', 141),\n",
       " ('darren', 141),\n",
       " ('field', 140),\n",
       " ('suite', 140),\n",
       " ('dollars', 139),\n",
       " ('data', 139),\n",
       " ('deliveries', 139),\n",
       " ('remove', 138),\n",
       " ('around', 137),\n",
       " ('including', 137),\n",
       " ('mike', 137),\n",
       " ('hope', 136),\n",
       " ('section', 136),\n",
       " ('three', 136),\n",
       " ('never', 135),\n",
       " ('spreadsheet', 135),\n",
       " ('sell', 134),\n",
       " ('party', 134),\n",
       " ('wanted', 134),\n",
       " ('wednesday', 133),\n",
       " ('nominations', 133),\n",
       " ('everyone', 132),\n",
       " ('cost', 132),\n",
       " ('complete', 132),\n",
       " ('brian', 132),\n",
       " ('tomorrow', 132),\n",
       " ('gathering', 132),\n",
       " ('offers', 131),\n",
       " ('confirm', 131),\n",
       " ('problems', 131),\n",
       " ('changed', 131),\n",
       " ('issues', 131),\n",
       " ('move', 131),\n",
       " ('something', 131),\n",
       " ('always', 130),\n",
       " ('quality', 130),\n",
       " ('transaction', 130),\n",
       " ('marketing', 130),\n",
       " ('carlos', 130),\n",
       " ('beginning', 129),\n",
       " ('scheduling', 129),\n",
       " ('customers', 128),\n",
       " ('risk', 128),\n",
       " ('scheduled', 128),\n",
       " ('rita', 128),\n",
       " ('eastrans', 127),\n",
       " ('necessary', 126),\n",
       " ('news', 126),\n",
       " ('direct', 125),\n",
       " ('limited', 125),\n",
       " ('type', 125),\n",
       " ('plan', 125),\n",
       " ('jackie', 125),\n",
       " ('tickets', 125),\n",
       " ('form', 124),\n",
       " ('allocated', 124),\n",
       " ('hplc', 124),\n",
       " ('software', 123),\n",
       " ('full', 123),\n",
       " ('allen', 123),\n",
       " ('person', 122),\n",
       " ('2004', 122),\n",
       " ('global', 122),\n",
       " ('interest', 122),\n",
       " ('stone', 122),\n",
       " ('follows', 122),\n",
       " ('hanks', 122),\n",
       " ('wish', 121),\n",
       " ('real', 121),\n",
       " ('howard', 121),\n",
       " ('aimee', 121),\n",
       " ('young', 120),\n",
       " ('zero', 120),\n",
       " ('anyone', 120),\n",
       " ('accounting', 120),\n",
       " ('fuels', 120),\n",
       " ('unify', 120),\n",
       " ('required', 119),\n",
       " ('page', 119),\n",
       " ('error', 119),\n",
       " ('firm', 119),\n",
       " ('meet', 119),\n",
       " ('wynne', 119),\n",
       " ('demand', 118),\n",
       " ('else', 118),\n",
       " ('things', 118),\n",
       " ('pricing', 118),\n",
       " ('million', 117),\n",
       " ('charlie', 117),\n",
       " ('spot', 117),\n",
       " ('security', 116),\n",
       " ('provided', 116),\n",
       " ('results', 116),\n",
       " ('easy', 115),\n",
       " ('past', 115),\n",
       " ('requested', 115),\n",
       " ('sorry', 115),\n",
       " ('several', 114),\n",
       " ('found', 114),\n",
       " ('performance', 114),\n",
       " ('added', 113),\n",
       " ('city', 113),\n",
       " ('rodriguez', 113),\n",
       " ('called', 112),\n",
       " ('cover', 112),\n",
       " ('addition', 112),\n",
       " ('ever', 112),\n",
       " ('thought', 112),\n",
       " ('6353', 112),\n",
       " ('unsubscribe', 111),\n",
       " ('love', 111),\n",
       " ('different', 111),\n",
       " ('enter', 110),\n",
       " ('logistics', 110),\n",
       " ('microsoft', 109),\n",
       " ('utilities', 109),\n",
       " ('camp', 109),\n",
       " ('iferc', 109),\n",
       " ('stacey', 108),\n",
       " ('second', 108),\n",
       " ('power', 108),\n",
       " ('loss', 107),\n",
       " ('investment', 107),\n",
       " ('amount', 107),\n",
       " ('area', 107),\n",
       " ('third', 107),\n",
       " ('thru', 107),\n",
       " ('valley', 107),\n",
       " ('hplno', 107),\n",
       " ('reason', 106),\n",
       " ('stock', 106),\n",
       " ('operations', 106),\n",
       " ('shows', 106),\n",
       " ('handle', 106),\n",
       " ('katy', 106),\n",
       " ('important', 105),\n",
       " ('removed', 105),\n",
       " ('exchange', 105),\n",
       " ('therefore', 105),\n",
       " ('swing', 105),\n",
       " ('remember', 104),\n",
       " ('small', 104),\n",
       " ('action', 104),\n",
       " ('years', 104),\n",
       " ('private', 104),\n",
       " ('together', 104),\n",
       " ('clear', 104),\n",
       " ('1999', 104),\n",
       " ('companies', 103),\n",
       " ('longer', 103),\n",
       " ('valid', 103),\n",
       " ('case', 103),\n",
       " ('basis', 103),\n",
       " ('interested', 103),\n",
       " ('portfolio', 103),\n",
       " ('financial', 103),\n",
       " ('probably', 103),\n",
       " ('left', 103),\n",
       " ('times', 103),\n",
       " ('listed', 102),\n",
       " ('canada', 102),\n",
       " ('cotton', 102),\n",
       " ('acton', 102),\n",
       " ('sept', 102),\n",
       " ('donald', 102),\n",
       " ('pops', 102),\n",
       " ('flowed', 102),\n",
       " ('weekend', 101),\n",
       " ('cash', 101),\n",
       " ('life', 101),\n",
       " ('question', 101),\n",
       " ('position', 101),\n",
       " ('project', 100),\n",
       " ('everything', 100),\n",
       " ('return', 100),\n",
       " ('close', 100),\n",
       " ('prescription', 99),\n",
       " ('commercial', 99),\n",
       " ('professional', 99),\n",
       " ('intended', 99),\n",
       " ('payment', 99),\n",
       " ('really', 99),\n",
       " ('ready', 98),\n",
       " ('states', 98),\n",
       " ('text', 98),\n",
       " ('html', 98),\n",
       " ('book', 98),\n",
       " ('included', 98),\n",
       " ('meds', 98),\n",
       " ('aware', 98),\n",
       " ('schedule', 98),\n",
       " ('zone', 98),\n",
       " ('coming', 98),\n",
       " ('afternoon', 98),\n",
       " ('lloyd', 98),\n",
       " ('latest', 97),\n",
       " ('producer', 97),\n",
       " ('weeks', 97),\n",
       " ('updated', 97),\n",
       " ('response', 96),\n",
       " ('rates', 96),\n",
       " ('talk', 96),\n",
       " ('invoice', 96),\n",
       " ('open', 95),\n",
       " ('bill', 95),\n",
       " ('view', 95),\n",
       " ('letter', 95),\n",
       " ('kind', 95),\n",
       " ('clem', 95),\n",
       " ('hplo', 95),\n",
       " ('tell', 94),\n",
       " ('program', 94),\n",
       " ('extended', 94),\n",
       " ('operating', 94),\n",
       " ('dave', 94),\n",
       " ('terms', 93),\n",
       " ('credit', 93),\n",
       " ('browser', 93),\n",
       " ('later', 93),\n",
       " ('trying', 93),\n",
       " ('donna', 93),\n",
       " ('pipe', 93),\n",
       " ('extend', 93),\n",
       " ('redeliveries', 93),\n",
       " ('viagra', 92),\n",
       " ('shipping', 92),\n",
       " ('given', 92),\n",
       " ('late', 92),\n",
       " ('told', 92),\n",
       " ('international', 91),\n",
       " ('plus', 91),\n",
       " ('index', 91),\n",
       " ('center', 91),\n",
       " ('large', 91),\n",
       " ('thomas', 91),\n",
       " ('reinhardt', 91),\n",
       " ('enserch', 91),\n",
       " ('begin', 90),\n",
       " ('hard', 90),\n",
       " ('mailto', 90),\n",
       " ('watch', 90),\n",
       " ('completed', 90),\n",
       " ('department', 90),\n",
       " ('mentioned', 90),\n",
       " ('dfarmer', 90),\n",
       " ('directly', 89),\n",
       " ('dear', 89),\n",
       " ('discuss', 89),\n",
       " ('side', 89),\n",
       " ('fact', 89),\n",
       " ('asked', 89),\n",
       " ('level', 88),\n",
       " ('cause', 88),\n",
       " ('allow', 88),\n",
       " ('taking', 88),\n",
       " ('wants', 88),\n",
       " ('nothing', 88),\n",
       " ('though', 88),\n",
       " ('spoke', 88),\n",
       " ('bellamy', 88),\n",
       " ('lauri', 88),\n",
       " ('location', 87),\n",
       " ('charge', 87),\n",
       " ('size', 87),\n",
       " ('delivered', 87),\n",
       " ('expected', 87),\n",
       " ('quick', 87),\n",
       " ('paste', 87),\n",
       " ('brenda', 87),\n",
       " ('katherine', 87),\n",
       " ('website', 86),\n",
       " ('related', 86),\n",
       " ('immediately', 86),\n",
       " ('united', 86),\n",
       " ('state', 86),\n",
       " ('corporation', 86),\n",
       " ('ensure', 86),\n",
       " ('systems', 86),\n",
       " ('lannou', 86),\n",
       " ('capital', 85),\n",
       " ('windows', 85),\n",
       " ('night', 85),\n",
       " ('receipt', 85),\n",
       " ('short', 85),\n",
       " ('scott', 85),\n",
       " ('cynthia', 85),\n",
       " ('answer', 85),\n",
       " ('path', 85),\n",
       " ('legal', 84),\n",
       " ('technology', 84),\n",
       " ('paliourg', 84),\n",
       " ('starting', 84),\n",
       " ('expect', 84),\n",
       " ('development', 84),\n",
       " ('showing', 84),\n",
       " ('hello', 83),\n",
       " ('approximately', 83),\n",
       " ('includes', 83),\n",
       " ('care', 83),\n",
       " ('employees', 83),\n",
       " ('estimates', 83),\n",
       " ('edward', 83),\n",
       " ('referenced', 83),\n",
       " ('ranch', 83),\n",
       " ('additionally', 83),\n",
       " ('revision', 83),\n",
       " ('status', 82),\n",
       " ('version', 82),\n",
       " ('mobile', 82),\n",
       " ('write', 81),\n",
       " ('assistance', 81),\n",
       " ('taken', 81),\n",
       " ('union', 81),\n",
       " ('monthly', 81),\n",
       " ('general', 81),\n",
       " ('verify', 81),\n",
       " ('central', 81),\n",
       " ('approval', 81),\n",
       " ('allocation', 81),\n",
       " ('voice', 81),\n",
       " ('jeff', 81),\n",
       " ('avila', 81),\n",
       " ('upon', 80),\n",
       " ('considered', 80),\n",
       " ('items', 80),\n",
       " ('south', 80),\n",
       " ('chance', 80),\n",
       " ('compliance', 80),\n",
       " ('fast', 79),\n",
       " ('star', 79),\n",
       " ('effort', 79),\n",
       " ('thing', 79),\n",
       " ('pain', 79),\n",
       " ('reference', 79),\n",
       " ('guys', 79),\n",
       " ('east', 79),\n",
       " ('plans', 78),\n",
       " ('title', 78),\n",
       " ('major', 78),\n",
       " ('bring', 78),\n",
       " ('happy', 78),\n",
       " ('confirmed', 78),\n",
       " ('janet', 78),\n",
       " ('bryan', 78),\n",
       " ('base', 77),\n",
       " ('federal', 77),\n",
       " ('fill', 77),\n",
       " ('final', 77),\n",
       " ('industry', 77),\n",
       " ('reflect', 77),\n",
       " ('looks', 77),\n",
       " ('carthage', 77),\n",
       " ('early', 77),\n",
       " ('understanding', 76),\n",
       " ('statements', 76),\n",
       " ('computer', 76),\n",
       " ('sold', 76),\n",
       " ('attention', 76),\n",
       " ('balance', 76),\n",
       " ('certain', 76),\n",
       " ('priced', 76),\n",
       " ('staff', 76),\n",
       " ('cernosek', 76),\n",
       " ('potential', 75),\n",
       " ('solution', 75),\n",
       " ('2005', 75),\n",
       " ('personal', 75),\n",
       " ('requirements', 75),\n",
       " ('nominated', 75),\n",
       " ('buyback', 75),\n",
       " ('baumbach', 75),\n",
       " ('enough', 74),\n",
       " ('country', 74),\n",
       " ('others', 74),\n",
       " ('drugs', 74),\n",
       " ('making', 74),\n",
       " ('public', 74),\n",
       " ('notes', 74),\n",
       " ('passed', 74),\n",
       " ('fixed', 74),\n",
       " ('experience', 74),\n",
       " ('simply', 74),\n",
       " ('took', 74),\n",
       " ('went', 74),\n",
       " ('parker', 74),\n",
       " ('sherlyn', 74),\n",
       " ('reliantenergy', 74),\n",
       " ('growth', 73),\n",
       " ('value', 73),\n",
       " ('advice', 73),\n",
       " ('moved', 73),\n",
       " ('shown', 73),\n",
       " ('application', 73),\n",
       " ('print', 73),\n",
       " ('settlements', 73),\n",
       " ('enronxgate', 73),\n",
       " ('confirmation', 72),\n",
       " ('matter', 72),\n",
       " ('registered', 72),\n",
       " ('sign', 72),\n",
       " ('head', 72),\n",
       " ('become', 72),\n",
       " ('cialis', 72),\n",
       " ('takes', 72),\n",
       " ('distribution', 72),\n",
       " ('white', 72),\n",
       " ('purchased', 72),\n",
       " ('hall', 72),\n",
       " ('approved', 71),\n",
       " ('source', 71),\n",
       " ('hear', 71),\n",
       " ('beaumont', 71),\n",
       " ('instructions', 71),\n",
       " ('entire', 71),\n",
       " ('identified', 71),\n",
       " ('listing', 71),\n",
       " ('storage', 71),\n",
       " ('neal', 71),\n",
       " ('calpine', 71),\n",
       " ('herod', 71),\n",
       " ('eileen', 71),\n",
       " ('release', 70),\n",
       " ('content', 70),\n",
       " ('specific', 70),\n",
       " ('unit', 70),\n",
       " ('stocks', 70),\n",
       " ('whole', 70),\n",
       " ('hesse', 70),\n",
       " ('secure', 69),\n",
       " ('mind', 69),\n",
       " ('ability', 69),\n",
       " ('shut', 69),\n",
       " ('worldwide', 69),\n",
       " ('away', 69),\n",
       " ('drive', 69),\n",
       " ('property', 69),\n",
       " ('material', 69),\n",
       " ('user', 69),\n",
       " ('appreciate', 69),\n",
       " ('couple', 69),\n",
       " ('earl', 69),\n",
       " ('asap', 69),\n",
       " ('weissman', 69),\n",
       " ('ponton', 69),\n",
       " ('turn', 68),\n",
       " ('contains', 68),\n",
       " ('press', 68),\n",
       " ('four', 68),\n",
       " ('transportation', 68),\n",
       " ('says', 68),\n",
       " ('yesterday', 68),\n",
       " ('johnson', 68),\n",
       " ('cheryl', 68),\n",
       " ('williams', 68),\n",
       " ('transfer', 67),\n",
       " ('cheap', 67),\n",
       " ('family', 67),\n",
       " ('wide', 67),\n",
       " ('half', 67),\n",
       " ('partners', 67),\n",
       " ('understand', 67),\n",
       " ('securities', 67),\n",
       " ('spam', 67),\n",
       " ('availability', 67),\n",
       " ('occur', 67),\n",
       " ('none', 67),\n",
       " ('representative', 67),\n",
       " ('saturday', 67),\n",
       " ('schumack', 67),\n",
       " ('entex', 67),\n",
       " ('recent', 66),\n",
       " ('respect', 66),\n",
       " ('previously', 66),\n",
       " ('whether', 66),\n",
       " ('king', 66),\n",
       " ('green', 66),\n",
       " ('provides', 66),\n",
       " ('share', 65),\n",
       " ('opportunity', 65),\n",
       " ('track', 65),\n",
       " ('2003', 65),\n",
       " ('waiting', 65),\n",
       " ('pass', 65),\n",
       " ('format', 65),\n",
       " ('options', 65),\n",
       " ('excess', 65),\n",
       " ('director', 65),\n",
       " ('exactly', 65),\n",
       " ('record', 65),\n",
       " ('according', 65),\n",
       " ('statement', 65),\n",
       " ('planning', 65),\n",
       " ('respective', 65),\n",
       " ('advised', 65),\n",
       " ('comments', 65),\n",
       " ('victor', 65),\n",
       " ('cornhusker', 65),\n",
       " ('body', 64),\n",
       " ('ship', 64),\n",
       " ('assist', 64),\n",
       " ('receiving', 64),\n",
       " ('brand', 64),\n",
       " ('leave', 64),\n",
       " ('health', 64),\n",
       " ('remaining', 64),\n",
       " ('existing', 64),\n",
       " ('william', 64),\n",
       " ('louisiana', 64),\n",
       " ('discussions', 64),\n",
       " ('informed', 64),\n",
       " ('associated', 64),\n",
       " ('feedback', 64),\n",
       " ('77002', 64),\n",
       " ('near', 63),\n",
       " ('enjoy', 63),\n",
       " ('choose', 63),\n",
       " ('newsletter', 63),\n",
       " ('stay', 63),\n",
       " ('soft', 63),\n",
       " ('came', 63),\n",
       " ('rest', 63),\n",
       " ('safe', 63),\n",
       " ('flowing', 63),\n",
       " ('points', 63),\n",
       " ('purchases', 63),\n",
       " ('mobil', 63),\n",
       " ('physical', 63),\n",
       " ('imbalance', 63),\n",
       " ('lamphier', 63),\n",
       " ('strangers', 63),\n",
       " ('partner', 62),\n",
       " ('sincerely', 62),\n",
       " ('selected', 62),\n",
       " ('super', 62),\n",
       " ('join', 62),\n",
       " ('learn', 62),\n",
       " ('perfect', 62),\n",
       " ('president', 62),\n",
       " ('events', 62),\n",
       " ('estimate', 62),\n",
       " ('result', 62),\n",
       " ('suggestions', 62),\n",
       " ('jennifer', 62),\n",
       " ('yahoo', 62),\n",
       " ('kevin', 62),\n",
       " ('midcon', 62),\n",
       " ('tisdale', 62),\n",
       " ('thousand', 61),\n",
       " ('dealer', 61),\n",
       " ('records', 61),\n",
       " ('lose', 61),\n",
       " ('tracked', 61),\n",
       " ('similar', 61),\n",
       " ('estimated', 61),\n",
       " ('held', 61),\n",
       " ('expired', 61),\n",
       " ('9497', 61),\n",
       " ('poorman', 61),\n",
       " ('doctor', 60),\n",
       " ('concerns', 60),\n",
       " ('richard', 60),\n",
       " ('outstanding', 60),\n",
       " ('notify', 60),\n",
       " ('plain', 60),\n",
       " ('sunday', 60),\n",
       " ('started', 60),\n",
       " ('investing', 60),\n",
       " ('impact', 60),\n",
       " ('draft', 60),\n",
       " ('history', 60),\n",
       " ('heard', 60),\n",
       " ('jill', 60),\n",
       " ('terry', 60),\n",
       " ('lone', 60),\n",
       " ('anita', 60),\n",
       " ('bank', 59),\n",
       " ('guarantee', 59),\n",
       " ('respond', 59),\n",
       " ('confidential', 59),\n",
       " ('various', 59),\n",
       " ('download', 59),\n",
       " ('word', 59),\n",
       " ('karen', 59),\n",
       " ('committed', 59),\n",
       " ('paragraph', 59),\n",
       " ('selling', 59),\n",
       " ('fred', 59),\n",
       " ('hplnl', 59),\n",
       " ('determine', 58),\n",
       " ('appear', 58),\n",
       " ('option', 58),\n",
       " ('communication', 58),\n",
       " ('reach', 58),\n",
       " ('eric', 58),\n",
       " ('correctly', 58),\n",
       " ('neuweiler', 58),\n",
       " ('pathed', 58),\n",
       " ('riley', 58),\n",
       " ('cleburne', 58),\n",
       " ('agreed', 57),\n",
       " ('running', 57),\n",
       " ('known', 57),\n",
       " ('offering', 57),\n",
       " ('success', 57),\n",
       " ('costs', 57),\n",
       " ('channel', 57),\n",
       " ('projects', 57),\n",
       " ('active', 57),\n",
       " ('server', 57),\n",
       " ('seek', 57),\n",
       " ('featured', 57),\n",
       " ('appropriate', 57),\n",
       " ('mckay', 57),\n",
       " ('superty', 57),\n",
       " ('paso', 57),\n",
       " ('lower', 56),\n",
       " ('course', 56),\n",
       " ('express', 56),\n",
       " ('network', 56),\n",
       " ('mccoy', 56),\n",
       " ('wait', 56),\n",
       " ('meaning', 56),\n",
       " ('discussion', 56),\n",
       " ('bridge', 56),\n",
       " ('involved', 56),\n",
       " ('apache', 56),\n",
       " ('ricky', 56),\n",
       " ('walker', 56),\n",
       " ('equistar', 56),\n",
       " ('least', 55),\n",
       " ('door', 55),\n",
       " ('owners', 55),\n",
       " ('huge', 55),\n",
       " ('government', 55),\n",
       " ('five', 55),\n",
       " ('differ', 55),\n",
       " ('reports', 55),\n",
       " ('sending', 55),\n",
       " ('instead', 55),\n",
       " ('document', 55),\n",
       " ('apply', 55),\n",
       " ('vacation', 55),\n",
       " ('processing', 55),\n",
       " ('situation', 55),\n",
       " ('hakemack', 55),\n",
       " ('involve', 54),\n",
       " ('comes', 54),\n",
       " ('generic', 54),\n",
       " ('weight', 54),\n",
       " ('conditions', 54),\n",
       " ('means', 54),\n",
       " ('friends', 54),\n",
       " ('require', 54),\n",
       " ('remain', 54),\n",
       " ('industrial', 54),\n",
       " ('morgan', 54),\n",
       " ('allocate', 54),\n",
       " ('recently', 54),\n",
       " ('along', 54),\n",
       " ('transactions', 54),\n",
       " ('svcs', 54),\n",
       " ('holmes', 54),\n",
       " ('reserves', 54),\n",
       " ('shipped', 53),\n",
       " ('members', 53),\n",
       " ('assets', 53),\n",
       " ('password', 53),\n",
       " ('facility', 53),\n",
       " ('opinion', 53),\n",
       " ('recorded', 53),\n",
       " ('goes', 53),\n",
       " ('creative', 53),\n",
       " ('stephanie', 53),\n",
       " ('dell', 53),\n",
       " ('suggest', 53),\n",
       " ('lots', 53),\n",
       " ('archer', 53),\n",
       " ('papayoti', 53),\n",
       " ('medical', 52),\n",
       " ('increased', 52),\n",
       " ('medications', 52),\n",
       " ('pills', 52),\n",
       " ('fine', 52),\n",
       " ('card', 52),\n",
       " ('features', 52),\n",
       " ('bought', 52),\n",
       " ('2002', 52),\n",
       " ('exploration', 52),\n",
       " ('hill', 52),\n",
       " ('xanax', 52),\n",
       " ('chad', 52),\n",
       " ('users', 52),\n",
       " ('oasis', 52),\n",
       " ('duty', 52),\n",
       " ('summary', 52),\n",
       " ('beverly', 52),\n",
       " ('placed', 52),\n",
       " ('methanol', 52),\n",
       " ('light', 51),\n",
       " ('risks', 51),\n",
       " ('store', 51),\n",
       " ('image', 51),\n",
       " ('play', 51),\n",
       " ('corporate', 51),\n",
       " ('adobe', 51),\n",
       " ('strong', 51),\n",
       " ('welcome', 51),\n",
       " ('files', 51),\n",
       " ('discussed', 51),\n",
       " ('member', 51),\n",
       " ('words', 51),\n",
       " ('presently', 51),\n",
       " ('sheet', 51),\n",
       " ('purposes', 51),\n",
       " ('paying', 51),\n",
       " ('expectations', 51),\n",
       " ('hour', 51),\n",
       " ('roll', 51),\n",
       " ('earlier', 51),\n",
       " ('morris', 51),\n",
       " ('opportunities', 51),\n",
       " ('conversation', 51),\n",
       " ...]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_tokens = nltk.FreqDist([token.lower() for tokenlist in df['tokens'].values for token in set(tokenlist) if len(token)>=4 and all_words[token]>=10 and token not in stopwd])\n",
    "print(len(relevant_tokens))\n",
    "top_relevant_tokens = relevant_tokens.most_common(2500)\n",
    "top_relevant_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in top_relevant_tokens:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "fset = [(d_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
    "random.shuffle(fset)\n",
    "len(fset)\n",
    "train, test = fset[:4139], fset[4139:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_KmDBHwiy8"
   },
   "source": [
    "2. **Validación del modelo anterior:**  \n",
    "---\n",
    "\n",
    "una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "AM6Vhy-Fw8oj"
   },
   "outputs": [],
   "source": [
    "new_classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7018393030009681\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(new_classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "  contains(('0200', 23)) = False             ham : spam   =      1.0 : 1.0\n",
      "  contains(('0500', 20)) = False             ham : spam   =      1.0 : 1.0\n",
      "  contains(('0600', 22)) = False             ham : spam   =      1.0 : 1.0\n",
      "  contains(('1000', 29)) = False             ham : spam   =      1.0 : 1.0\n",
      "  contains(('1200', 37)) = False             ham : spam   =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "new_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lC72_CbxAoJ"
   },
   "source": [
    "3. **Construye mejores atributos**: A veces no solo se trata de las palabras más frecuentes sino de el contexto, y capturar contexto no es posible solo viendo los tokens de forma individual, ¿que tal si consideramos bi-gramas, tri-gramas ...?, ¿las secuencias de palabras podrián funcionar como mejores atributos para el modelo?. Para ver si es así,  podemos extraer n-gramas de nuestro corpus y obtener sus frecuencias de aparición con `FreqDist()`, desarrolla tu propia manera de hacerlo y entrena un modelo con esos nuevos atributos, no olvides compartir tus resultados en la sección de comentarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "wtMkQWpfxoy3"
   },
   "outputs": [],
   "source": [
    "# Calculamos todos los bigramas del corpus:\n",
    "threshold = 2\n",
    "md_bigrams = []\n",
    "for tokenlist in df['tokens'].values:\n",
    "    md_bigrams += [bigram for bigram in list(nltk.bigrams(tokenlist)) if len(bigram[0])>threshold] \n",
    "\n",
    "md_bigrams_fdist = nltk.FreqDist(md_bigrams)\n",
    "common_bigrams = dict(md_bigrams_fdist.most_common(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    document_bigrams = list(nltk.bigrams(document))\n",
    "    for word in top_relevant_tokens:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    for bigram in common_bigrams:\n",
    "        features['contains_bigram({})'.format(bigram)] = (bigram in document_bigrams)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "fset = [(d2_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
    "random.shuffle(fset)\n",
    "len(fset)\n",
    "train, test = fset[:4139], fset[4139:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classifier2 = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8731848983543078\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(new_classifier2, test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copia de [Lecture_19/20]Modelos_clasificacion.ipynb",
   "provenance": [
    {
     "file_id": "1KnTJeBTqTLpdWpBT8PFLiOhGskBlq0fb",
     "timestamp": 1629882775588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
