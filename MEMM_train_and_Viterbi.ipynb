{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "24ad6a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sumit\\\\OneDrive\\\\Escritorio\\\\Universidad\\\\P_Curriculares\\\\Clasificación de texto (NLP)'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/sumit/OneDrive/Escritorio/Universidad/P_Curriculares/Clasificación de texto (NLP)/venv/Lib/site-packages\")\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b347494",
   "metadata": {},
   "source": [
    "# Entrenando un Modelo Markoviano de máxima entropía\n",
    "\n",
    "## Entrenamiento del modelo - Cálculo de Conteos\n",
    "\n",
    "- P(ti | wi,ti-1) = C(wi/ti/ti-1) / C(wi/ti-1)\n",
    "- word_tags = C(wi/ti/ti-1)\n",
    "- word_prevtag = C(wi/ti-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "856845e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "\n",
    "word_tags = {}\n",
    "word_prevtag = {}\n",
    "\n",
    "tagtype = 'upos'\n",
    "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for tokenlist in parse_incr(data_file):\n",
    "    prevtag = \"None\"\n",
    "    for token in tokenlist:\n",
    "        tag = token[tagtype]\n",
    "        \n",
    "        # Conteo de C(wi|ti|ti-1)\n",
    "        wordtags = token['form'].lower() + \"|\" + tag + \"|\" + prevtag\n",
    "        \n",
    "        if wordtags in word_tags.keys():\n",
    "            word_tags[wordtags] += 1\n",
    "        else:\n",
    "            word_tags[wordtags] = 1\n",
    "            \n",
    "        # Conteo de C(wi|ti-1)\n",
    "        wordprevtag = token['form'].lower() + \"|\" + prevtag\n",
    "        if wordprevtag in word_prevtag.keys():\n",
    "            word_prevtag[wordprevtag] += 1\n",
    "        else:\n",
    "            word_prevtag[wordprevtag] = 1\n",
    "                \n",
    "        prevtag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8bf2196a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463363"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(word_prevtag.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e1f9a",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo - calculo de probabilidades\n",
    "\n",
    "- P(ti|wi,ti-1) = C(wi|ti|ti-1) / C(wi|ti-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "326a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memmProbs = {}\n",
    "\n",
    "# Cálculo de las probabilidades:\n",
    "for key in word_tags.keys():\n",
    "    x = key.split(\"|\")\n",
    "    if len(x) > 3:\n",
    "        new_key = x[2] + \"|\" + \"|,\" + x[3]\n",
    "        memmProbs[new_key] = word_tags[key] / word_prevtag[\"|\"+\"|\"+x[3]]\n",
    "    else:\n",
    "        new_key = x[1] + \"|\" + x[0] + \",\" + x[2]\n",
    "        memmProbs[new_key] = word_tags[key] / word_prevtag[x[0]+\"|\"+x[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e9b6f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029239766081871343\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "memmProbs\n",
    "print(memmProbs['ADV|mientras,PUNCT'])\n",
    "print(memmProbs['NUM|miles,ADP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7adb7",
   "metadata": {},
   "source": [
    "## Algoritmo de Viterbi para MEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cabe09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identificamos las categorias gramaticales 'upos' unicas en el corpus\n",
    "stateSet = {'ADJ',\n",
    " 'ADP',\n",
    " 'ADV',\n",
    " 'AUX',\n",
    " 'CCONJ',\n",
    " 'DET',\n",
    " 'INTJ',\n",
    " 'NOUN',\n",
    " 'NUM',\n",
    " 'PART',\n",
    " 'PRON',\n",
    " 'PROPN',\n",
    " 'PUNCT',\n",
    " 'SCONJ',\n",
    " 'SYM',\n",
    " 'VERB',\n",
    " '_'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6141c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': 0,\n",
       " 'PRON': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'CCONJ': 4,\n",
       " 'INTJ': 5,\n",
       " 'NUM': 6,\n",
       " 'ADJ': 7,\n",
       " 'AUX': 8,\n",
       " 'VERB': 9,\n",
       " 'PART': 10,\n",
       " 'DET': 11,\n",
       " 'SCONJ': 12,\n",
       " 'PUNCT': 13,\n",
       " '_': 14,\n",
       " 'SYM': 15,\n",
       " 'PROPN': 16}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enumeramos las categorias con numeros para asignar a \n",
    "# las filas de la matriz de Viterbi\n",
    "tagStateDict = {}\n",
    "for i, state in enumerate(stateSet):\n",
    "  tagStateDict[state] = i\n",
    "tagStateDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57591ccb",
   "metadata": {},
   "source": [
    "## Distribución inicial de estados latentes\n",
    "\n",
    "Para poder calcular la distribución inicial de estados, tenemos que sacar las probabilidades de que al principio de la frase aparezca alguna categoría gramatical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6243584b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DET': 0.34799021321216356,\n",
       " 'ADP': 0.14931842013282068,\n",
       " 'VERB': 0.04557846906675987,\n",
       " 'ADV': 0.07577770010485844,\n",
       " 'PROPN': 0.10506815798671792,\n",
       " 'PRON': 0.04173365955959455,\n",
       " 'ADJ': 0.010136315973435861,\n",
       " 'CCONJ': 0.036980076896190144,\n",
       " 'PART': 0.002446696959105208,\n",
       " 'PUNCT': 0.09143656064313177,\n",
       " 'NOUN': 0.025026214610276126,\n",
       " 'NUM': 0.0068507514854945824,\n",
       " '_': 0.009227542817196784,\n",
       " 'SCONJ': 0.027123383432366307,\n",
       " 'AUX': 0.022789234533379936,\n",
       " 'INTJ': 0.0020272631946871723,\n",
       " 'SYM': 0.0004893393918210416}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculamos distribución inicial de estados\n",
    "initTagStateProb = {} # \\rho_i^{(0)}\n",
    "from conllu import parse_incr \n",
    "\n",
    "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "count = 0 # cuenta la longitud del corpus\n",
    "for tokenlist in parse_incr(data_file):\n",
    "  count += 1\n",
    "  tag = tokenlist[0]['upos']\n",
    "  if tag in initTagStateProb.keys():\n",
    "    initTagStateProb[tag] += 1\n",
    "  else:\n",
    "    initTagStateProb[tag] = 1\n",
    "\n",
    "for key in initTagStateProb.keys():\n",
    "  initTagStateProb[key] /= count\n",
    "\n",
    "initTagStateProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1f34261c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificamos que la suma de las probabilidades es 1 (100%)\n",
    "import numpy as np\n",
    "np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e5d87",
   "metadata": {},
   "source": [
    "## Construcción del algoritmo de Viterbi para MEMM\n",
    "\n",
    "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c}\n",
    "\\begin{array}{c c c c}\n",
    "\\text{ADJ} \\\\\n",
    "\\text{ADV}\\\\\n",
    "\\text{PRON} \\\\\n",
    "\\vdots \\\\\n",
    "{}\n",
    "\\end{array} \n",
    "&\n",
    "\\left[\n",
    "\\begin{array}{c c c c}\n",
    "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
    "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
    "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
    "p_1 & p_2 & \\dots & p_n \n",
    "\\end{array}\n",
    "\\right] \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Donde las probabilidades de Viterbi en la primera columna (para una categoria $i$) están dadas por: \n",
    "\n",
    "$$\n",
    "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times P(i \\vert p_1, \\text{\"None\"})\n",
    "$$\n",
    "\n",
    "y para las siguientes columnas: \n",
    "\n",
    "$$\n",
    "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times P(j \\vert p_t, i) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0e2b53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a00daa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViterbiMatrix(secuencia, memmProbs=memmProbs,\n",
    "                  tagStateDict=tagStateDict,\n",
    "                  initTagStateProb=initTagStateProb):\n",
    "    seq = word_tokenize(secuencia)\n",
    "    viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
    "\n",
    "    # inicialización primera columna\n",
    "    for key in tagStateDict.keys():\n",
    "        tag_row = tagStateDict[key]\n",
    "        word_tags = key + \"|\" + seq[0].lower() + \",\" + \"None\"\n",
    "        if word_tags in memmProbs.keys():\n",
    "            viterbiProb[tag_row, 0] = initTagStateProb[key]*memmProbs[word_tags]\n",
    "    \n",
    "    # computo de las siguientes columnas\n",
    "    for col in range(1, len(seq)):\n",
    "        for key in tagStateDict.keys():\n",
    "            tag_row = tagStateDict[key]\n",
    "            possible_probs = []\n",
    "            for key2 in tagStateDict.keys():\n",
    "                word_tags = key + \"|\" + seq[col].lower() + \",\" + key2\n",
    "                tag_row2 = tagStateDict[key2]\n",
    "            \n",
    "                if word_tags in memmProbs.keys():\n",
    "                    # miramos estados de la col anterior\n",
    "                    if viterbiProb[tag_row2, col-1]>0:\n",
    "                        possible_probs.append(viterbiProb[tag_row2, col-1]*memmProbs[word_tags])\n",
    "            \n",
    "            if len(possible_probs) > 0:\n",
    "                viterbiProb[tag_row, col] = max(possible_probs)\n",
    "\n",
    "    return viterbiProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "93eddf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 3.32592416e-01, 5.82360262e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.26768814e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 3.26768814e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.47990213e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.53977970e-02, 2.12872770e-04, 0.00000000e+00]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = ViterbiMatrix('el mundo es pequeño')\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24de483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViterbiTags(secuencia, memmProbs=memmProbs,\n",
    "                  tagStateDict=tagStateDict,\n",
    "                  initTagStateProb=initTagStateProb):\n",
    "    seq = word_tokenize(secuencia)\n",
    "    viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
    "\n",
    "    # inicialización primera columna\n",
    "    for key in tagStateDict.keys():\n",
    "        tag_row = tagStateDict[key]\n",
    "        word_tags = key + \"|\" + seq[0].lower() + \",\" + \"None\"\n",
    "        if word_tags in memmProbs.keys():\n",
    "            viterbiProb[tag_row, 0] = initTagStateProb[key]*memmProbs[word_tags]\n",
    "    \n",
    "    # computo de las siguientes columnas\n",
    "    for col in range(1, len(seq)):\n",
    "        for key in tagStateDict.keys():\n",
    "            tag_row = tagStateDict[key]\n",
    "            possible_probs = []\n",
    "            for key2 in tagStateDict.keys():\n",
    "                word_tags = key + \"|\" + seq[col].lower() + \",\" + key2\n",
    "                tag_row2 = tagStateDict[key2]\n",
    "            \n",
    "                if word_tags in memmProbs.keys():\n",
    "                    # miramos estados de la col anterior\n",
    "                    if viterbiProb[tag_row2, col-1]>0:\n",
    "                        possible_probs.append(viterbiProb[tag_row2, col-1]*memmProbs[word_tags])\n",
    "            \n",
    "            if len(possible_probs) > 0:\n",
    "                viterbiProb[tag_row, col] = max(possible_probs)\n",
    "    \n",
    "    # contruccion de secuencia de tags\n",
    "    res = []\n",
    "    for i, p in enumerate(seq):\n",
    "        for tag in tagStateDict.keys():\n",
    "            if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
    "                res.append((p, tag))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3bf3c9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('el', 'DET'),\n",
       " ('mundo', 'NOUN'),\n",
       " ('es', 'AUX'),\n",
       " ('muy', 'ADV'),\n",
       " ('pequeño', 'ADJ')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViterbiTags('el mundo es muy pequeño')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9ae39e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estos', 'DET'),\n",
       " ('instrumentos', 'NOUN'),\n",
       " ('han', 'AUX'),\n",
       " ('de', 'ADP'),\n",
       " ('rasgar', 'NOUN')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViterbiTags('estos instrumentos han de rasgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0baf7105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yo', 'PRON'), ('me', 'PRON'), ('llamo', 'VERB'), ('Sumit', 'NOUN')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViterbiTags('Yo me llamo Sumit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134051de",
   "metadata": {},
   "source": [
    "### ¿ Siguientes Pasos ? \n",
    "\n",
    "El modelo construido, aunque es la base de un MEMM, no explota todo el potencial del concepto  que estos modelos representan, en nuestro caso sencillo consideramos solo un **feature** para predecir la categoría gramatical: $<w_i, t_{i-1}>$. Es decir, las probabilidades de una cierta etiqueta $t_i$ dada una observación $<w_i, t_{i-1}>$ se calculan contando eventos donde se observe que $<w_i, t_{i-1}>$ sucede simultáneamente con $t_i$. \n",
    "\n",
    "La generalización de esto (donde puedo considerar multiples observaciones o **features**, y a partir de estos inferir la categoría gramatical) se hace construyendo las llamadas **feature-functions**, donde estas funciones toman valores de 0 o 1, cuando se cumplan las condiciones de la observación o feature en cuestion. En general podemos considerar una **feature-function** como : \n",
    "\n",
    "$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n",
    "\\begin{cases}\n",
    "  1 , & \\text{se cumple condición } a \\\\\n",
    "  0, & \\text{en caso contrario}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde la condición $a$ es una relacion entre los valores que tome $\\text{tag}$ y $\\text{context}$, por ejemplo:\n",
    "\n",
    "$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n",
    "\\begin{cases}\n",
    "  1 , & (t_i, t_{i-1}) = \\text{('VERB', 'ADJ')} \\\\\n",
    "  0, & \\text{en caso contrario}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Al considerar varias funciones, y por lo tanto varios features observables, consideramos una combinacion lineal de estos por medio de un coeficiente que multiplique a cada función: \n",
    "\n",
    "$$\n",
    "\\theta_1 f_1(t, o) + \\theta_2 f_2(t, o) + \\dots\n",
    "$$\n",
    "\n",
    "donde los coeficientes indicarán cuales features son más relevantes y por lo tanto pesan más para la decisión del resultado del modelo. De esta manera los coeficientes $\\theta_j$ se vuelven parámetros del modelo que deben ser optimizados (esto puede realizarse con cualquier técnica de optimizacion como el Gradiente Descendente). Ahora, las probabilidades que pueden obtener usando un softmax sobre estas combinaciones lineales de features: \n",
    "\n",
    "$$\n",
    "P = \\prod_i \\frac{\\exp{\\left(\\sum_j \\theta_j f_j(t_i, o)\\right)}}{\\sum_{t'}\\exp{\\left(\\sum_j \\theta_j f_j(t', o)\\right)}}\n",
    "$$\n",
    "\n",
    "Así, lo que buscamos con el algoritmo de optimización es encontrar los parámetros $\\theta_j$ que maximizan la probabilidad anterior. En NLTK encontramos la implementación completa de un clasificador de máxima entropia que no esta restringido a relaciones markovianas: https://www.nltk.org/_modules/nltk/classify/maxent.html\n",
    "\n",
    "Un segmento resumido de la clase en python que implementa este clasificador en NLTK lo encuentras así: \n",
    "\n",
    "```\n",
    "class MaxentClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, encoding, weights, logarithmic=True):\n",
    "        self._encoding = encoding\n",
    "        self._weights = weights\n",
    "        self._logarithmic = logarithmic\n",
    "        assert encoding.length() == len(weights)\n",
    "\n",
    "    def labels(self):\n",
    "        return self._encoding.labels()\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        self._weights = new_weights\n",
    "        assert self._encoding.length() == len(new_weights)\n",
    "\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def classify(self, featureset):\n",
    "        return self.prob_classify(featureset).max()\n",
    "\n",
    "    def prob_classify(self, featureset):\n",
    "        ### ...\n",
    "\n",
    "        # Normalize the dictionary to give a probability distribution\n",
    "        return DictionaryProbDist(prob_dict, log=self._logarithmic, normalize=True)\n",
    "\n",
    "    @classmethod\n",
    "    def train(\n",
    "        cls,\n",
    "        train_toks,\n",
    "        algorithm=None,\n",
    "        trace=3,\n",
    "        encoding=None,\n",
    "        labels=None,\n",
    "        gaussian_prior_sigma=0,\n",
    "        **cutoffs\n",
    "    ):\n",
    "     ### ......\n",
    "```\n",
    "\n",
    "Donde te das cuenta de la forma que tienen las clases en NLTK que implementan clasificadores generales. Aquí vemos que la clase `MaxentClassifier` es una subclase de una más general `ClassifierI` la cual representa el proceso de clasificación general de categoría única (es decir, que a cada data-point le corresponda solo una categoria), también que esta clase depende de definir un `encoding`\n",
    " y unos pesos `weights` : \n",
    "\n",
    "```\n",
    "class MaxentClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, encoding, weights, logarithmic=True):\n",
    "```\n",
    "\n",
    "los pesos corresponden a los parámetros $\\theta_i$. Y el encoding es el que corresponde a las funciones $f_a(t, o)$ que dan como resultado valores binarios $1$ o $0$.\n",
    "\n",
    "La documentación de NLTK te puede dar mas detalles de esta implementación: https://www.nltk.org/api/nltk.classify.html\n",
    "\n",
    "Finalmente, un ejemplo completo de uso y mejora de un modelo de máxima entropía, lo puedes encontrar en este fork que guarde especialmente para el curso, para que lo tengas de referencia y puedas jugar y aprender con él: \n",
    "\n",
    "https://github.com/pachocamacho1990/nltk-maxent-pos-tagger\n",
    "\n",
    "El cual fue desarrollado originalmente por Arne Neumann (https://github.com/arne-cl) basado en los fueatures propuestos por Ratnaparki en 1996 para la tarea de etiquetado por categorias gramaticales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
