{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YTaQq9dCh0E"
   },
   "source": [
    "# Clasificación de palabras (por género de nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sumit\\\\OneDrive\\\\Escritorio\\\\Universidad\\\\P_Curriculares\\\\Clasificación de texto (NLP)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/sumit/OneDrive/Escritorio/Universidad/P_Curriculares/Clasificación de texto (NLP)/venv/Lib/site-packages\")\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1594920888413,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "5co_TuOhC4ze",
    "outputId": "6ed198ee-9cb9-48b9-ab74-db9655585c0e"
   },
   "outputs": [],
   "source": [
    "import nltk, random\n",
    "#nltk.download('names')\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijhsE2PBFYxm"
   },
   "source": [
    "**Función básica de extracción de atributos**\n",
    "\n",
    "Vamos a hacer que al principio esta función devuelva solamente como atributo la última letra del nombre de la persona ya que los humanos somos capaces de clasificar un nombre como masculino o femenino teniendo en cuenta solo este atributo.\n",
    "\n",
    "PD: el corpus es un conjunto de nombre en **inglés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "b0kKV62lCZ55"
   },
   "outputs": [],
   "source": [
    "# definición de atributos relevantes\n",
    "def atributos(palabra):\n",
    "    return {'ultima_letra': palabra[-1]}\n",
    "\n",
    "tagset = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(APRENDIZAJE SUPERVISADO)\n",
    "\n",
    "Pero esta lista tiene todos los nombres masculinos primero y luego los femeninos, lo cuál puede ser un problema al momento de entrenar el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1594920888415,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "IjfK5ZKwDL__",
    "outputId": "fef1000d-e474-43f0-986d-10b721946e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aamir', 'male'),\n",
       " ('Aaron', 'male'),\n",
       " ('Abbey', 'male'),\n",
       " ('Abbie', 'male'),\n",
       " ('Abbot', 'male'),\n",
       " ('Abbott', 'male'),\n",
       " ('Abby', 'male'),\n",
       " ('Abdel', 'male'),\n",
       " ('Abdul', 'male'),\n",
       " ('Abdulkarim', 'male')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1594920888416,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "jZcAN-dmCrok",
    "outputId": "b114852f-8e71-4121-fda3-21bb731353b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Agneta', 'female'),\n",
       " ('Dugan', 'male'),\n",
       " ('Tobi', 'female'),\n",
       " ('Axel', 'male'),\n",
       " ('Patrice', 'female'),\n",
       " ('Renault', 'male'),\n",
       " ('Rudd', 'male'),\n",
       " ('Chelsy', 'female'),\n",
       " ('Petra', 'female'),\n",
       " ('Wojciech', 'male')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(tagset)\n",
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos una lista con pares: (atributos,genero) ya que el modelo que estamos creando no lee los nombres sino los atributos de los nombres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QzK97C8BDmHR"
   },
   "outputs": [],
   "source": [
    "fset = [(atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQPv0tW4Fd2G"
   },
   "source": [
    "**Modelo de clasificación Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "37jueg4nDQFs"
   },
   "outputs": [],
   "source": [
    "# entrenamiento del modelo NaiveBayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAWfUSHrEj3q"
   },
   "source": [
    " **Verificación de algunas predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1594920888418,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "Mr8ytm8SEEZk",
    "outputId": "cf62ff8a-2722-4331-bf70-66aafff1d9e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('amanda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1594920888418,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "c0GG1Y1_EPaO",
    "outputId": "6f286792-7845-44f8-b22b-1cf6815036a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('peter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea no tiene porque demostrar que esta clasificando bien, hay que mirar el performance del modelo que hemos entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(atributos('sumit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSUK14XhEqLL"
   },
   "source": [
    "**Performance del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1594920888419,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "lenwC5agEdvT",
    "outputId": "1ceb5e52-4db6-4c5f-c714-dbf72f90e652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy:  0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"test_accuracy: \", nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1594920888822,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "p5S9qeCgsJSg",
    "outputId": "6339e65d-d66e-4c96-9053-ea70d0abdea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy:  0.7610155830198818\n"
     ]
    }
   ],
   "source": [
    "print(\"train_accuracy: \", nltk.classify.accuracy(classifier, train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSNI7OFxGib0"
   },
   "source": [
    "**Mejores atributos**\n",
    "\n",
    "¿Pero como podemos coger mejores atributos para intentar aumentar la precisión del modelo? Vamos a hacerlo por prueba y error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "k5uaIAdDGlq8"
   },
   "outputs": [],
   "source": [
    "def mas_atributos(nombre):\n",
    "    atrib = {}\n",
    "    atrib[\"primera_letra\"] = nombre[0].lower()\n",
    "    atrib[\"ultima_letra\"] = nombre[-1].lower()\n",
    "    for letra in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrib[\"count({})\".format(letra)] = nombre.lower().count(letra)\n",
    "        atrib[\"has({})\".format(letra)] = (letra in nombre.lower())\n",
    "    return atrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6-gJIxKcHKvI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'primera_letra': 'j',\n",
       " 'ultima_letra': 'n',\n",
       " 'count(a)': 0,\n",
       " 'has(a)': False,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mas_atributos('jhon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FBu25HHgHQtK"
   },
   "outputs": [],
   "source": [
    "fset = [(mas_atributos(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 2127,
     "status": "ok",
     "timestamp": 1594920889465,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "8hWR9hOzHlNe",
    "outputId": "97d5f514-dfd5-474a-8755-4127a7bcb8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy:  0.796\n"
     ]
    }
   ],
   "source": [
    "print(\"test_accuracy: \", nltk.classify.accuracy(classifier2, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio de práctica\n",
    "\n",
    "**Objetivo:** Construye un classificador de nombres en español usando el siguiente dataset: \n",
    "https://github.com/jvalhondo/spanish-names-surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Preparación de los datos**: con un `git clone` puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset `names` de nombres en ingles. \n",
    "\n",
    "* **Piensa y analiza**: ¿los features en ingles aplican de la misma manera para los nombres en español?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación de los datos:\n",
    "\n",
    "import csv\n",
    "\n",
    "f_file, m_file = open('datasets/Spanish_Names/female_names.csv'), open('datasets/Spanish_Names/male_names.csv')\n",
    "f_reader, m_reader = csv.reader(f_file), csv.reader(m_file)\n",
    "\n",
    "# Ignoramos las cabeceras:\n",
    "next(f_reader)\n",
    "next(m_reader)\n",
    "\n",
    "tagset = ([(row[0], 'male') for row in m_reader] + [(row[0], 'female') for row in f_reader])\n",
    "random.shuffle(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ANDREW RICHARD', 'male'),\n",
       " ('NABI', 'male'),\n",
       " ('DORA LILIA', 'female'),\n",
       " ('RIE', 'female'),\n",
       " ('GALDER', 'male'),\n",
       " ('AITANA', 'female'),\n",
       " ('BLANCA SOLEDAD', 'female'),\n",
       " ('WEIMING', 'male'),\n",
       " ('PRADEEP', 'male'),\n",
       " ('RICARDO EMANUEL', 'male')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Entrenamiento y performance del modelo**: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3*. **Mejores atributos:** Define una función como `atributos2()` donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cogemos primero como atributo la última letra de un nombre español:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_1(name):\n",
    "    return {'last_letter': name[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.7893529893529894\n",
      "test_accuracy:  0.828\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_1(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_1('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_1('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_1('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_1('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_1('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_1('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_1('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_1('Jose María')))\n",
    "print(c1_esp.classify(atr_1('Carlos')))\n",
    "print(c1_esp.classify(atr_1('Javier')))\n",
    "print(c1_esp.classify(atr_1('Javier Rey')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a coger más atributos ahora:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_2(name):\n",
    "    atrs = {}\n",
    "    atrs[\"first_letter\"] = name[0].lower()\n",
    "    atrs[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8004095004095004\n",
      "test_accuracy:  0.804\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_2(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_2('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_2('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_2('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_2('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_2('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_2('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_2('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_2('Jose María')))\n",
    "print(c1_esp.classify(atr_2('Carlos')))\n",
    "print(c1_esp.classify(atr_2('Javier')))\n",
    "print(c1_esp.classify(atr_2('Javier Rey')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_3(name):\n",
    "    atrs = {}\n",
    "    name_surnames = name.split(' ')\n",
    "    \n",
    "    ## Getting first and last letter from name:\n",
    "    atrs[\"first_letter_name\"] = name_surnames[0][0].lower()\n",
    "    atrs[\"last_letter_name\"] = name_surnames[0][-1].lower()\n",
    "    \n",
    "    ## Getting first and last letter from first surname (if has):\n",
    "    if len(name_surnames) > 1:\n",
    "        atrs[\"first_letter_firstSurname\"] = name_surnames[1][0].lower()\n",
    "        atrs[\"last_letter_firstSurname\"] = name_surnames[1][-1].lower()\n",
    "    \n",
    "    ## Getting first and last letter from Second surname (if has):\n",
    "    if len(name_surnames) > 2:\n",
    "        atrs[\"first_letter_secondSurname\"] = name_surnames[2][0].lower()\n",
    "        atrs[\"last_letter_secondSurname\"] = name_surnames[2][-1].lower()\n",
    "    \n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letter_name': 's',\n",
       " 'last_letter_name': 't',\n",
       " 'first_letter_firstSurname': 'k',\n",
       " 'last_letter_firstSurname': 'r',\n",
       " 'first_letter_secondSurname': 'j',\n",
       " 'last_letter_secondSurname': 'i',\n",
       " 'count(a)': 2,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 1,\n",
       " 'has(e)': True,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 2,\n",
       " 'has(i)': True,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 1,\n",
       " 'has(k)': True,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 2,\n",
       " 'has(m)': True,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 0,\n",
       " 'has(o)': False,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 1,\n",
       " 'has(r)': True,\n",
       " 'count(s)': 1,\n",
       " 'has(s)': True,\n",
       " 'count(t)': 2,\n",
       " 'has(t)': True,\n",
       " 'count(u)': 2,\n",
       " 'has(u)': True,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atr_3(\"Sumit Kumar Jethani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8506756756756757\n",
      "test_accuracy:  0.842\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_3(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_3('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_3('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_3('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_3('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_3('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_3('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_3('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_3('Jose María')))\n",
    "print(c1_esp.classify(atr_3('Carlos')))\n",
    "print(c1_esp.classify(atr_3('Javier')))\n",
    "print(c1_esp.classify(atr_3('Javier Rey')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_4(name):\n",
    "    atrs = {}\n",
    "    name_surnames = name.split(' ')\n",
    "    \n",
    "    ## Getting the two first and last letters from name:\n",
    "    atrs[\"first_letters_name\"] = name_surnames[0][:2].lower()\n",
    "    atrs[\"last_letters_name\"] = name_surnames[0][-2:].lower()\n",
    "    \n",
    "    ## Getting the two first and last letters from first surname (if has):\n",
    "    if len(name_surnames) > 1:\n",
    "        atrs[\"first_letters_firstSurname\"] = name_surnames[1][:2].lower()\n",
    "        atrs[\"last_letters_firstSurname\"] = name_surnames[1][-2:].lower()\n",
    "    \n",
    "    ## Getting the two first and last letters from Second surname (if has):\n",
    "    if len(name_surnames) > 2:\n",
    "        atrs[\"first_letters_secondSurname\"] = name_surnames[2][:2].lower()\n",
    "        atrs[\"last_letters_secondSurname\"] = name_surnames[2][-2:].lower()\n",
    "    \n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        atrs[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        atrs[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return atrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_letters_name': 'su',\n",
       " 'last_letters_name': 'mi',\n",
       " 'first_letters_firstSurname': 'ku',\n",
       " 'last_letters_firstSurname': 'ar',\n",
       " 'first_letters_secondSurname': 'je',\n",
       " 'last_letters_secondSurname': 'ni',\n",
       " 'count(a)': 2,\n",
       " 'has(a)': True,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 1,\n",
       " 'has(e)': True,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 2,\n",
       " 'has(i)': True,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 1,\n",
       " 'has(k)': True,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 2,\n",
       " 'has(m)': True,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 0,\n",
       " 'has(o)': False,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 1,\n",
       " 'has(r)': True,\n",
       " 'count(s)': 1,\n",
       " 'has(s)': True,\n",
       " 'count(t)': 1,\n",
       " 'has(t)': True,\n",
       " 'count(u)': 2,\n",
       " 'has(u)': True,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atr_4(\"Sumi Kumar Jethani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy:  0.8886363636363637\n",
      "test_accuracy:  0.872\n"
     ]
    }
   ],
   "source": [
    "fset = [(atr_4(n), g) for (n, g) in tagset]\n",
    "train, test = fset[500:], fset[:500]\n",
    "c1_esp = nltk.NaiveBayesClassifier.train(train)\n",
    "print(\"training_accuracy: \", nltk.classify.accuracy(c1_esp, train))\n",
    "print(\"test_accuracy: \", nltk.classify.accuracy(c1_esp, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "female\n",
      "female\n",
      "male\n",
      "male\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print(c1_esp.classify(atr_4('Ricardo Arbelo')))\n",
    "print(c1_esp.classify(atr_4('Victoria Montelongo Martín')))\n",
    "print(c1_esp.classify(atr_4('Matias Ortega Sarmiento')))\n",
    "print(c1_esp.classify(atr_4('Victor Alejandro Santana')))\n",
    "print(c1_esp.classify(atr_4('Sumit Kumar Jethani')))\n",
    "print(c1_esp.classify(atr_4('Paula Ramos Ahumada')))\n",
    "print(c1_esp.classify(atr_4('María Gonzalez')))\n",
    "print(c1_esp.classify(atr_4('Jose María')))\n",
    "print(c1_esp.classify(atr_4('Carlos')))\n",
    "print(c1_esp.classify(atr_4('Javier')))\n",
    "print(c1_esp.classify(atr_4('Laura')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos seguir asi probando distintos atributos y esto es conocido como: **íngeniería de atributos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7CXFyfoGf4s"
   },
   "source": [
    "# Clasificación de documentos (email spam o no spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 3628,
     "status": "ok",
     "timestamp": 1594920890983,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "Qfli08sgIzl_",
    "outputId": "7b634f01-0956-4843-8eb3-a4e5d2c77d3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'datasets'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pachocamacho1990/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 3626,
     "status": "ok",
     "timestamp": 1594920890985,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "bHFKXxclJ5LC",
    "outputId": "e2878b62-45ed-482a-faa2-92ce90b87731"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 4259,
     "status": "ok",
     "timestamp": 1594920891622,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "33oKcvcjKrlM",
    "outputId": "6183550d-66c9-41a4-e2a6-52c9da7ec461"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>contenido</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>[&lt;, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&gt; Russell Turpin:\\r\\n&gt; &gt; That depends on how t...</td>\n",
       "      <td>[&gt;, Russell, Turpin, :, &gt;, &gt;, That, depends, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Help wanted.  We are a 14 year old fortune 500...</td>\n",
       "      <td>[Help, wanted, ., We, are, a, 14, year, old, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Request A Free No Obligation Consultation!\\r\\n...</td>\n",
       "      <td>[Request, A, Free, No, Obligation, Consultatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Is there a way to look for a particular file o...</td>\n",
       "      <td>[Is, there, a, way, to, look, for, a, particul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clase                                          contenido  \\\n",
       "0     -1  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "1      1  > Russell Turpin:\\r\\n> > That depends on how t...   \n",
       "2     -1  Help wanted.  We are a 14 year old fortune 500...   \n",
       "3     -1  Request A Free No Obligation Consultation!\\r\\n...   \n",
       "4      1  Is there a way to look for a particular file o...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [<, !, DOCTYPE, HTML, PUBLIC, ``, -//W3C//DTD,...  \n",
       "1  [>, Russell, Turpin, :, >, >, That, depends, o...  \n",
       "2  [Help, wanted, ., We, are, a, 14, year, old, f...  \n",
       "3  [Request, A, Free, No, Obligation, Consultatio...  \n",
       "4  [Is, there, a, way, to, look, for, a, particul...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/email/csv/spam-apache.csv', names = ['clase','contenido'])\n",
    "df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OvHkYDylNMKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " '!',\n",
       " 'DOCTYPE',\n",
       " 'HTML',\n",
       " 'PUBLIC',\n",
       " '``',\n",
       " '-//W3C//DTD',\n",
       " 'HTML',\n",
       " '4.0',\n",
       " 'Transitional//EN',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'HTML',\n",
       " '>',\n",
       " '<',\n",
       " 'HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'http-equiv=Content-Type',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'text/html',\n",
       " ';',\n",
       " 'charset=iso-8859-1',\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'META',\n",
       " 'content=',\n",
       " \"''\",\n",
       " 'MSHTML',\n",
       " '6.00.2600.0',\n",
       " \"''\",\n",
       " 'name=GENERATOR',\n",
       " '>',\n",
       " '<',\n",
       " 'STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/STYLE',\n",
       " '>',\n",
       " '<',\n",
       " '/HEAD',\n",
       " '>',\n",
       " '<',\n",
       " 'BODY',\n",
       " 'bgColor=',\n",
       " '#',\n",
       " 'ffffff',\n",
       " '>',\n",
       " '<',\n",
       " 'DIV',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=Arial',\n",
       " 'size=2',\n",
       " '>',\n",
       " '<',\n",
       " 'FONT',\n",
       " 'face=',\n",
       " \"''\",\n",
       " 'Times',\n",
       " 'New',\n",
       " 'Roman',\n",
       " \"''\",\n",
       " 'size=3',\n",
       " '>',\n",
       " 'Dear',\n",
       " 'Friend',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'A',\n",
       " 'recent',\n",
       " 'survey',\n",
       " 'by',\n",
       " 'Nielsen/Netratings',\n",
       " 'says',\n",
       " 'that',\n",
       " '``',\n",
       " 'The',\n",
       " 'Internet',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'population',\n",
       " 'is',\n",
       " 'rapidly',\n",
       " 'approaching',\n",
       " 'a',\n",
       " \"'Half\",\n",
       " 'a',\n",
       " 'Billion',\n",
       " \"'\",\n",
       " 'people',\n",
       " '!',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'SO',\n",
       " 'WHAT',\n",
       " 'DOES',\n",
       " 'ALL',\n",
       " 'THIS',\n",
       " 'MEAN',\n",
       " 'TO',\n",
       " 'YOU',\n",
       " '?',\n",
       " 'EASY',\n",
       " 'MONEY',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'assume',\n",
       " 'that',\n",
       " 'every',\n",
       " 'person',\n",
       " 'has',\n",
       " 'only',\n",
       " 'one',\n",
       " 'E-mail',\n",
       " 'address',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'that',\n",
       " \"'s\",\n",
       " '500',\n",
       " 'million',\n",
       " 'potential',\n",
       " 'customers',\n",
       " 'and',\n",
       " 'growing',\n",
       " '!',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"E'mail\",\n",
       " 'is',\n",
       " 'without',\n",
       " 'question',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'method',\n",
       " 'of',\n",
       " 'distributing',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'I',\n",
       " 'think',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'picture',\n",
       " '.',\n",
       " 'The',\n",
       " 'numbers',\n",
       " 'and',\n",
       " 'potential',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'just',\n",
       " 'staggering',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'even',\n",
       " 'better',\n",
       " '...',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Suppose',\n",
       " 'I',\n",
       " 'told',\n",
       " 'you',\n",
       " 'that',\n",
       " 'you',\n",
       " 'could',\n",
       " 'start',\n",
       " 'your',\n",
       " 'own',\n",
       " 'E-mail',\n",
       " 'business',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'today',\n",
       " 'and',\n",
       " 'enjoy',\n",
       " 'these',\n",
       " 'benefits',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'All',\n",
       " 'Customers',\n",
       " 'Pay',\n",
       " 'You',\n",
       " 'In',\n",
       " 'Cash',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Will',\n",
       " 'Sell',\n",
       " 'A',\n",
       " 'Product',\n",
       " 'Which',\n",
       " 'Costs',\n",
       " 'Nothing',\n",
       " 'to',\n",
       " 'Produce',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'Your',\n",
       " 'Only',\n",
       " 'Overhead',\n",
       " 'Is',\n",
       " 'Your',\n",
       " 'Time',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Have',\n",
       " '100s',\n",
       " 'Of',\n",
       " 'Millions',\n",
       " 'Of',\n",
       " 'Potential',\n",
       " 'Customers',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'You',\n",
       " 'Get',\n",
       " 'Detailed',\n",
       " ',',\n",
       " 'Easy',\n",
       " 'To',\n",
       " 'Follow',\n",
       " 'Startup',\n",
       " 'Instructions',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THIS',\n",
       " 'IS',\n",
       " 'JUST',\n",
       " 'THE',\n",
       " 'TIP',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'ICEBERG',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'As',\n",
       " 'you',\n",
       " 'read',\n",
       " 'on',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'discover',\n",
       " 'how',\n",
       " 'a',\n",
       " \"'Seen\",\n",
       " 'on',\n",
       " 'National',\n",
       " 'TV',\n",
       " \"'\",\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'is',\n",
       " 'paying',\n",
       " 'out',\n",
       " 'a',\n",
       " 'half',\n",
       " 'million',\n",
       " 'dollars',\n",
       " ',',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " 'months',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'from',\n",
       " 'your',\n",
       " 'home',\n",
       " ',',\n",
       " 'for',\n",
       " 'an',\n",
       " 'investment',\n",
       " 'of',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'US',\n",
       " 'Dollars',\n",
       " 'expense',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'one',\n",
       " 'time',\n",
       " '.',\n",
       " 'ALL',\n",
       " 'THANKS',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'COMPUTER',\n",
       " 'AGE',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'THE',\n",
       " 'INTERNET',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Before',\n",
       " 'you',\n",
       " 'say',\n",
       " '``',\n",
       " 'Bull',\n",
       " \"''\",\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'you',\n",
       " 'have',\n",
       " 'been',\n",
       " 'hearing',\n",
       " 'about',\n",
       " 'on',\n",
       " 'the',\n",
       " 'news',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'lately',\n",
       " '.',\n",
       " 'Due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'this',\n",
       " 'letter',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Internet',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'a',\n",
       " 'national',\n",
       " 'weekly',\n",
       " 'news',\n",
       " 'program',\n",
       " 'recently',\n",
       " 'devoted',\n",
       " 'an',\n",
       " 'entire',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'show',\n",
       " 'to',\n",
       " 'the',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'this',\n",
       " 'program',\n",
       " 'described',\n",
       " 'below',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'to',\n",
       " 'see',\n",
       " 'if',\n",
       " 'it',\n",
       " 'really',\n",
       " 'can',\n",
       " 'make',\n",
       " 'people',\n",
       " 'money.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'show',\n",
       " 'also',\n",
       " 'investigated',\n",
       " 'whether',\n",
       " 'or',\n",
       " 'not',\n",
       " 'the',\n",
       " 'program',\n",
       " 'was',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'legal',\n",
       " '.',\n",
       " 'Their',\n",
       " 'findings',\n",
       " 'proved',\n",
       " 'once',\n",
       " 'and',\n",
       " 'for',\n",
       " 'all',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'absolutely',\n",
       " 'NO',\n",
       " 'laws',\n",
       " 'prohibiting',\n",
       " 'the',\n",
       " 'participation',\n",
       " 'in',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'program',\n",
       " 'and',\n",
       " 'if',\n",
       " 'people',\n",
       " 'can',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'they',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'make',\n",
       " 'some',\n",
       " 'mega',\n",
       " 'bucks',\n",
       " 'with',\n",
       " 'only',\n",
       " '$',\n",
       " '25',\n",
       " 'out',\n",
       " 'of',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'pocket',\n",
       " 'cost',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'DUE',\n",
       " 'TO',\n",
       " 'THE',\n",
       " 'RECENT',\n",
       " 'INCREASE',\n",
       " 'OF',\n",
       " 'POPULARITY',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'AND',\n",
       " 'RESPECT',\n",
       " 'THIS',\n",
       " 'PROGRAM',\n",
       " 'HAS',\n",
       " 'ATTAINED',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'IT',\n",
       " 'IS',\n",
       " 'CURRENTLY',\n",
       " 'WORKING',\n",
       " 'BETTER',\n",
       " 'THAN',\n",
       " 'EVER',\n",
       " '!',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " '*',\n",
       " 'This',\n",
       " 'is',\n",
       " 'what',\n",
       " 'one',\n",
       " 'had',\n",
       " 'to',\n",
       " 'say',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'Thanks',\n",
       " 'to',\n",
       " 'this',\n",
       " 'profitable',\n",
       " 'opportunity',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'approached',\n",
       " 'many',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'times',\n",
       " 'before',\n",
       " 'but',\n",
       " 'each',\n",
       " 'time',\n",
       " 'I',\n",
       " 'passed',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'so',\n",
       " 'glad',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'finally',\n",
       " 'joined',\n",
       " 'just',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'one',\n",
       " 'could',\n",
       " 'expect',\n",
       " 'in',\n",
       " 'return',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'for',\n",
       " 'the',\n",
       " 'minimal',\n",
       " 'effort',\n",
       " 'and',\n",
       " 'money',\n",
       " 'required',\n",
       " '.',\n",
       " 'To',\n",
       " 'my',\n",
       " 'asonishment',\n",
       " ',',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'I',\n",
       " 'received',\n",
       " 'total',\n",
       " '$',\n",
       " '610,470.00',\n",
       " 'in',\n",
       " '21',\n",
       " 'weeks',\n",
       " ',',\n",
       " 'with',\n",
       " 'money',\n",
       " 'still',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'coming',\n",
       " 'in',\n",
       " \"''\",\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Pam',\n",
       " 'Hedland',\n",
       " ',',\n",
       " 'Fort',\n",
       " 'Lee',\n",
       " ',',\n",
       " 'New',\n",
       " 'Jersey',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'another',\n",
       " 'testimonial',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " \"''\",\n",
       " 'This',\n",
       " 'program',\n",
       " 'has',\n",
       " 'been',\n",
       " 'around',\n",
       " 'for',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'but',\n",
       " 'I',\n",
       " 'never',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'believed',\n",
       " 'in',\n",
       " 'it',\n",
       " '.',\n",
       " 'But',\n",
       " 'one',\n",
       " 'day',\n",
       " 'when',\n",
       " 'I',\n",
       " 'received',\n",
       " 'this',\n",
       " 'again',\n",
       " 'in',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'the',\n",
       " 'mail',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'gamble',\n",
       " 'my',\n",
       " '$',\n",
       " '25',\n",
       " 'on',\n",
       " 'it',\n",
       " '.',\n",
       " 'I',\n",
       " 'followed',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'simple',\n",
       " 'instructions',\n",
       " 'and',\n",
       " 'walaa',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '3',\n",
       " 'weeks',\n",
       " 'later',\n",
       " 'the',\n",
       " 'money',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'started',\n",
       " 'to',\n",
       " 'come',\n",
       " 'in',\n",
       " '.',\n",
       " 'First',\n",
       " 'month',\n",
       " 'I',\n",
       " 'only',\n",
       " 'made',\n",
       " '$',\n",
       " '240.00',\n",
       " 'but',\n",
       " 'the',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'next',\n",
       " '2',\n",
       " 'months',\n",
       " 'after',\n",
       " 'that',\n",
       " 'I',\n",
       " 'made',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '$',\n",
       " '290,000.00.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'So',\n",
       " 'far',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '8',\n",
       " 'months',\n",
       " 'by',\n",
       " 're-entering',\n",
       " 'the',\n",
       " 'program',\n",
       " ',',\n",
       " 'I',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'have',\n",
       " 'made',\n",
       " 'over',\n",
       " '$',\n",
       " '710,000.00',\n",
       " 'and',\n",
       " 'I',\n",
       " 'am',\n",
       " 'playing',\n",
       " 'it',\n",
       " 'again.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'The',\n",
       " 'key',\n",
       " 'to',\n",
       " 'success',\n",
       " 'in',\n",
       " 'this',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'simple',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'steps',\n",
       " 'and',\n",
       " 'NOT',\n",
       " 'change',\n",
       " 'anything',\n",
       " '.',\n",
       " '``',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'More',\n",
       " 'testimonials',\n",
       " 'later',\n",
       " 'but',\n",
       " 'first',\n",
       " ':',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '*',\n",
       " '*',\n",
       " 'PRINT',\n",
       " 'THIS',\n",
       " 'NOW',\n",
       " 'FOR',\n",
       " 'YOUR',\n",
       " 'FUTURE',\n",
       " 'REFERENCE',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'If',\n",
       " 'you',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'make',\n",
       " 'at',\n",
       " 'least',\n",
       " '$',\n",
       " '500,000',\n",
       " 'every',\n",
       " '4',\n",
       " 'to',\n",
       " '5',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'months',\n",
       " 'easily',\n",
       " 'and',\n",
       " 'comfortably',\n",
       " ',',\n",
       " 'please',\n",
       " 'read',\n",
       " 'the',\n",
       " 'following',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<',\n",
       " 'BR',\n",
       " '>',\n",
       " 'THEN',\n",
       " 'READ',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O4kw1BQUOe4-"
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist])\n",
    "top_words = all_words.most_common(200)\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1g6F_qNfmRAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"contains((',', 2173))\": False,\n",
       " \"contains(('.', 2171))\": False,\n",
       " \"contains(('the', 1967))\": False,\n",
       " \"contains(('>', 1787))\": False,\n",
       " \"contains(('--', 1611))\": False,\n",
       " \"contains(('to', 1435))\": False,\n",
       " \"contains((':', 1220))\": False,\n",
       " \"contains(('*', 1149))\": False,\n",
       " \"contains(('and', 1064))\": False,\n",
       " \"contains(('of', 958))\": False,\n",
       " \"contains(('a', 879))\": False,\n",
       " \"contains(('you', 744))\": False,\n",
       " \"contains(('in', 742))\": False,\n",
       " \"contains(('I', 741))\": False,\n",
       " \"contains(('<', 718))\": False,\n",
       " \"contains(('!', 698))\": False,\n",
       " \"contains(('%', 677))\": False,\n",
       " \"contains(('for', 609))\": False,\n",
       " \"contains(('is', 578))\": False,\n",
       " \"contains(('#', 521))\": False,\n",
       " \"contains(('BR', 494))\": False,\n",
       " \"contains(('that', 479))\": False,\n",
       " \"contains((')', 463))\": False,\n",
       " \"contains(('it', 458))\": False,\n",
       " 'contains((\"\\'\\'\", 434))': False,\n",
       " \"contains(('$', 413))\": False,\n",
       " \"contains(('this', 384))\": False,\n",
       " \"contains(('(', 380))\": False,\n",
       " \"contains(('on', 378))\": False,\n",
       " \"contains(('http', 362))\": False,\n",
       " \"contains(('?', 360))\": False,\n",
       " \"contains(('your', 359))\": False,\n",
       " \"contains(('have', 351))\": False,\n",
       " \"contains(('with', 334))\": False,\n",
       " \"contains(('``', 307))\": False,\n",
       " \"contains(('be', 299))\": False,\n",
       " \"contains(('-', 289))\": False,\n",
       " \"contains(('from', 271))\": False,\n",
       " 'contains((\"\\'s\", 263))': False,\n",
       " \"contains(('are', 257))\": False,\n",
       " \"contains(('31', 255))\": False,\n",
       " \"contains(('or', 252))\": False,\n",
       " \"contains(('as', 251))\": False,\n",
       " \"contains(('will', 243))\": False,\n",
       " \"contains(('not', 226))\": False,\n",
       " \"contains(('30', 220))\": False,\n",
       " \"contains(('my', 206))\": False,\n",
       " \"contains(('at', 199))\": False,\n",
       " \"contains(('The', 198))\": False,\n",
       " \"contains(('has', 195))\": False,\n",
       " \"contains(('can', 194))\": False,\n",
       " \"contains(('&', 181))\": False,\n",
       " \"contains(('all', 176))\": False,\n",
       " 'contains((\"n\\'t\", 175))': False,\n",
       " \"contains(('do', 167))\": False,\n",
       " \"contains(('out', 166))\": False,\n",
       " \"contains(('but', 164))\": False,\n",
       " \"contains(('...', 160))\": False,\n",
       " \"contains(('our', 160))\": False,\n",
       " \"contains(('by', 156))\": False,\n",
       " \"contains(('if', 152))\": False,\n",
       " \"contains(('was', 149))\": False,\n",
       " \"contains(('one', 129))\": False,\n",
       " \"contains(('an', 129))\": False,\n",
       " \"contains(('just', 128))\": False,\n",
       " \"contains(('@', 128))\": False,\n",
       " \"contains(('This', 125))\": False,\n",
       " \"contains(('1', 123))\": False,\n",
       " \"contains(('If', 118))\": False,\n",
       " \"contains(('more', 118))\": False,\n",
       " \"contains(('You', 117))\": False,\n",
       " \"contains(('5', 116))\": False,\n",
       " \"contains(('we', 116))\": False,\n",
       " \"contains(('time', 114))\": False,\n",
       " \"contains(('people', 110))\": False,\n",
       " \"contains(('me', 110))\": False,\n",
       " \"contains(('We', 110))\": False,\n",
       " \"contains(('THE', 108))\": False,\n",
       " \"contains(('up', 108))\": False,\n",
       " \"contains(('get', 107))\": False,\n",
       " \"contains(('they', 103))\": False,\n",
       " \"contains(('only', 100))\": False,\n",
       " \"contains(('like', 100))\": False,\n",
       " \"contains(('so', 99))\": False,\n",
       " 'contains((\"\\'\", 95))': False,\n",
       " \"contains(('To', 95))\": False,\n",
       " \"contains(('list', 95))\": False,\n",
       " \"contains(('2', 94))\": False,\n",
       " \"contains(('other', 92))\": False,\n",
       " \"contains(('A', 91))\": False,\n",
       " \"contains(('FREE', 90))\": False,\n",
       " \"contains(('No', 90))\": False,\n",
       " \"contains(('would', 88))\": False,\n",
       " \"contains(('any', 88))\": False,\n",
       " \"contains(('been', 87))\": False,\n",
       " \"contains(('who', 87))\": False,\n",
       " \"contains(('there', 86))\": False,\n",
       " \"contains(('which', 86))\": False,\n",
       " \"contains(('|', 84))\": False,\n",
       " \"contains(('about', 81))\": False,\n",
       " \"contains((']', 81))\": False,\n",
       " \"contains(('some', 80))\": False,\n",
       " \"contains(('email', 80))\": False,\n",
       " \"contains(('what', 79))\": False,\n",
       " \"contains(('AND', 77))\": False,\n",
       " \"contains(('their', 76))\": False,\n",
       " \"contains(('TO', 75))\": False,\n",
       " \"contains(('no', 75))\": False,\n",
       " \"contains(('then', 74))\": False,\n",
       " \"contains(('his', 74))\": False,\n",
       " \"contains(('It', 74))\": False,\n",
       " \"contains(('address', 73))\": False,\n",
       " \"contains(('use', 73))\": False,\n",
       " \"contains(('YOU', 72))\": False,\n",
       " \"contains(('money', 72))\": False,\n",
       " \"contains(('3', 72))\": False,\n",
       " \"contains(('[', 71))\": False,\n",
       " \"contains(('each', 70))\": False,\n",
       " \"contains(('work', 70))\": False,\n",
       " \"contains(('over', 69))\": False,\n",
       " \"contains(('he', 69))\": False,\n",
       " \"contains(('make', 68))\": False,\n",
       " \"contains(('send', 68))\": False,\n",
       " \"contains(('them', 68))\": False,\n",
       " \"contains(('OF', 67))\": False,\n",
       " \"contains(('name', 67))\": False,\n",
       " \"contains(('than', 67))\": False,\n",
       " \"contains(('2002', 67))\": False,\n",
       " \"contains(('could', 66))\": False,\n",
       " \"contains(('am', 66))\": False,\n",
       " \"contains(('unseen', 65))\": False,\n",
       " \"contains(('see', 63))\": False,\n",
       " \"contains(('YOUR', 63))\": False,\n",
       " \"contains(('4', 61))\": False,\n",
       " \"contains(('how', 60))\": False,\n",
       " \"contains(('way', 60))\": False,\n",
       " \"contains(('msgs', 59))\": False,\n",
       " \"contains(('lists/l-k', 59))\": False,\n",
       " \"contains(('wrote', 58))\": False,\n",
       " \"contains(('also', 57))\": False,\n",
       " \"contains(('here', 57))\": False,\n",
       " \"contains(('Your', 56))\": False,\n",
       " \"contains(('mail', 56))\": False,\n",
       " \"contains(('receive', 56))\": False,\n",
       " \"contains(('go', 56))\": False,\n",
       " \"contains(('program', 55))\": False,\n",
       " \"contains(('On', 55))\": False,\n",
       " \"contains(('new', 55))\": False,\n",
       " \"contains(('had', 54))\": False,\n",
       " \"contains(('NOT', 54))\": False,\n",
       " \"contains(('does', 54))\": False,\n",
       " \"contains(('want', 54))\": False,\n",
       " \"contains(('please', 53))\": False,\n",
       " \"contains(('us', 53))\": False,\n",
       " \"contains(('because', 53))\": False,\n",
       " \"contains(('REPORT', 52))\": False,\n",
       " \"contains(('below', 51))\": False,\n",
       " \"contains(('when', 51))\": False,\n",
       " \"contains(('e-mail', 51))\": False,\n",
       " 'contains((\"\\'m\", 51))': False,\n",
       " \"contains(('free', 50))\": False,\n",
       " \"contains(('think', 49))\": False,\n",
       " \"contains(('now', 49))\": False,\n",
       " \"contains(('first', 48))\": False,\n",
       " \"contains(('Please', 48))\": False,\n",
       " \"contains(('most', 47))\": False,\n",
       " \"contains(('within', 47))\": False,\n",
       " \"contains(('even', 46))\": False,\n",
       " \"contains(('using', 46))\": False,\n",
       " \"contains(('e-mails', 45))\": False,\n",
       " \"contains(('sent', 45))\": False,\n",
       " \"contains(('THIS', 44))\": False,\n",
       " \"contains(('In', 44))\": False,\n",
       " \"contains(('received', 44))\": False,\n",
       " \"contains(('company', 44))\": False,\n",
       " \"contains(('need', 43))\": False,\n",
       " \"contains(('much', 43))\": False,\n",
       " \"contains(('nbsp=3B', 43))\": False,\n",
       " \"contains(('did', 42))\": False,\n",
       " \"contains(('And', 42))\": False,\n",
       " \"contains(('still', 41))\": False,\n",
       " \"contains(('FOR', 41))\": False,\n",
       " 'contains((\"\\'re\", 41))': False,\n",
       " \"contains(('know', 41))\": False,\n",
       " \"contains((';', 40))\": False,\n",
       " \"contains(('=', 40))\": False,\n",
       " \"contains(('days', 40))\": False,\n",
       " \"contains(('where', 40))\": False,\n",
       " \"contains(('//www.adclick.ws/p.cfm', 40))\": False,\n",
       " \"contains(('line', 40))\": False,\n",
       " \"contains(('information', 39))\": False,\n",
       " \"contains(('US', 39))\": False,\n",
       " \"contains(('through', 39))\": False,\n",
       " \"contains(('message', 39))\": False,\n",
       " \"contains(('Linux', 39))\": False,\n",
       " 'contains((\"\\'ve\", 39))': False,\n",
       " \"contains(('made', 38))\": False,\n",
       " \"contains(('different', 38))\": False,\n",
       " \"contains(('those', 38))\": False,\n",
       " \"contains(('Report', 38))\": False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_features(df['tokens'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SrCXXGMCn3zz"
   },
   "outputs": [],
   "source": [
    "fset = [(document_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]\n",
    "random.shuffle(fset)\n",
    "train, test = fset[:200], fset[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r6FGZE4OqkEa"
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1594921479920,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "xIyVc6lBrGOy",
    "outputId": "01479a1e-6f68-4423-d4d3-6b1f9673f43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 4539,
     "status": "ok",
     "timestamp": 1594920891932,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "1x-R_PImrKIV",
    "outputId": "3b384c67-1640-422f-d9a1-19e41b42c667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "     contains((\"'\", 95)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"''\", 434)) = False              -1 : 1      =      1.0 : 1.0\n",
      "    contains((\"'m\", 51)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'re\", 41)) = False              -1 : 1      =      1.0 : 1.0\n",
      "   contains((\"'s\", 263)) = False              -1 : 1      =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1594921643207,
     "user": {
      "displayName": "Francisco Camacho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9-RzPl8GwmwlgVzviB9WCCmO7S-tSRs4UBCgR=s64",
      "userId": "03320326049189164988"
     },
     "user_tz": 300
    },
    "id": "wKzgha92up3l",
    "outputId": "ec079b59-5973-4084-e6ab-2f66037e92e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...\n",
       "2      Help wanted.  We are a 14 year old fortune 500...\n",
       "3      Request A Free No Obligation Consultation!\\r\\n...\n",
       "10     >\\r\\n>“µ×è¹µÑÇ ¡ÑºâÅ¡¸ØÃ¡Ô¨º¹ÍÔ¹àµÍÃìà¹çµ” \\r\\...\n",
       "11     ==============================================...\n",
       "                             ...                        \n",
       "243    ##############################################...\n",
       "244    Wanna see sexually curious teens playing with ...\n",
       "246    REQUEST FOR URGENT BUSINESS ASSISTANCE\\r\\n----...\n",
       "248    Email marketing works!  There's no way around ...\n",
       "249    Email marketing works!  There's no way around ...\n",
       "Name: contenido, Length: 125, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['clase']==-1]['contenido']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBvifrnr3GY"
   },
   "source": [
    "## Ejercicio de práctica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR53vedlvd1O"
   },
   "source": [
    "¿Como podrías construir un mejor clasificador de documentos?\n",
    "\n",
    "0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/` \n",
    "\n",
    "1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... . \n",
    "\n",
    "---\n",
    "\n",
    "Con base en eso construye un dataset más grande y con un tokenizado más pulido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOw2KrtnymVT"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2ZO0aJyrTLx"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_KmDBHwiy8"
   },
   "source": [
    "2. **Validación del modelo anterior:**  \n",
    "---\n",
    "\n",
    "una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM6Vhy-Fw8oj"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lC72_CbxAoJ"
   },
   "source": [
    "3. **Construye mejores atributos**: A veces no solo se trata de las palabras más frecuentes sino de el contexto, y capturar contexto no es posible solo viendo los tokens de forma individual, ¿que tal si consideramos bi-gramas, tri-gramas ...?, ¿las secuencias de palabras podrián funcionar como mejores atributos para el modelo?. Para ver si es así,  podemos extraer n-gramas de nuestro corpus y obtener sus frecuencias de aparición con `FreqDist()`, desarrolla tu propia manera de hacerlo y entrena un modelo con esos nuevos atributos, no olvides compartir tus resultados en la sección de comentarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtMkQWpfxoy3"
   },
   "outputs": [],
   "source": [
    "# escribe tu código aquí:\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copia de [Lecture_19/20]Modelos_clasificacion.ipynb",
   "provenance": [
    {
     "file_id": "1KnTJeBTqTLpdWpBT8PFLiOhGskBlq0fb",
     "timestamp": 1629882775588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
